<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Chris的个人博客</title>
  
  <subtitle>我的代码要为成艺术品</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2022-05-30T11:42:23.595Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Chris Peng</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>DeepLink</title>
    <link href="http://yoursite.com/2022/05/27/DeepLink/"/>
    <id>http://yoursite.com/2022/05/27/DeepLink/</id>
    <published>2022-05-27T10:16:59.000Z</published>
    <updated>2022-05-30T11:42:23.595Z</updated>
    
    <content type="html"><![CDATA[<h1 id="deeplink">DeepLink</h1><p><a id="more"></a></p><p>deeplink主要做了以下的事情：</p><p>（1）它对网络进行采样，在保持网络结构的同时生成训练“语料库”。</p><p>（2）网络中的每个节点通过网络嵌入被表示为低维空间中的一个向量；</p><p>（3）然后将锚节点输入深度神经网络，训练非线性变换，使用户跨网络对齐；</p><p>（4）DeepLink采用双学习过程来提高身份链接性能，提高监督训练算法。</p><p>首先在给定的一对网络a和一对网络B之间预先训练两个初步的映射函数，也就是<span class="math inline">\(\Phi(A \to B)\)</span>以及<span class="math inline">\(\Phi^{-1}(B \to A)\)</span>在没有标签（对齐）的锚节点上使用自动映射。然后，用户身份链接可以形式化为一个双重学习游戏：在网络A中的一个铆钉节点的嵌入表示<span class="math inline">\(\vec a\)</span>可以使用<span class="math inline">\(\Phi(A \to B)\)</span>得到一个在B中的对应向量<span class="math inline">\(\vec b&#39;\)</span>。随后向量<span class="math inline">\(\vec b&#39;\)</span>与其真实的向量<span class="math inline">\(\vec b\)</span>相比较，和A被通知<span class="math inline">\(\Phi(A \to B)\)</span>是否是一个高质量的映射，通过使用<span class="math inline">\(\Phi^{-1}(B \to A)\)</span>将<span class="math inline">\(\vec b&#39;\)</span>映射回去得到<span class="math inline">\(\vec a&#39;\)</span>。网络A测量<span class="math inline">\(\vec a\)</span>和<span class="math inline">\(\vec a&#39;\)</span>之间的相似度然后反馈给B。游戏也可以从B中的一个节点开始，并在B空间中嵌入向量。本质上，随着锚定节点的增加，这两个映射函数得到了越来越多的改进。</p><p><img src="/2022/05/27/DeepLink/deepwalk框架图.png"></p><p>经过随机游走采样得到语句<span class="math inline">\(S_{u_i}^r\)</span>之后，r表示第r轮次，deeplink使用skip-gram模型来更新embedding。</p><p><img src="/2022/05/27/DeepLink/skipgram优化方程.png"></p><p>这个公式就是skipgram的优化方程了，网络序列化的一个很重要的概念。w是滑动窗口的大小，条件概率的定义如下：</p><p><img src="/2022/05/27/DeepLink/条件概率公式.png"></p><p>采用负采样后的公式为：</p><p><img src="/2022/05/27/DeepLink/负采样-16538958028732.png"></p><p>在得到两个网络的embedding之后，deeplink通过MLP学习两个网络之间的映射函数，给定一组标记的anchor node pairs <span class="math inline">\((u_i,u_j)\)</span>以及其向量<span class="math inline">\((v(u_i),v(u_j))\)</span>,deeplink学习映射<span class="math inline">\(\Phi(v(u_i))\)</span>通过最小化如下函数：</p><p><img src="/2022/05/27/DeepLink/优化方程.png"></p><p>假设A和B是网络节点的向量矩阵，那么优化方程为：</p><p><img src="/2022/05/27/DeepLink/对齐的优化方程.png"></p><p>我们要学习的就是W和b，W是权重矩阵，b是偏置。</p><p>为了充分利用先验知识的anchor nodes，使用一套dual-learning的机制。假设我们有两个弱映射函数Φ和<span class="math inline">\(\Phi^{-1}\)</span>例如，用部分锚节点进行预训练-它们可以将向量从Gs投射到Gt，反之亦然。然后，我们改进了两个映射函数</p><ul><li>无监督UIL预训练：</li></ul><p>如果我们首先得到在<span class="math inline">\(G^s\)</span>中的anchor node的向量<span class="math inline">\(u_b\)</span>，先把他通过<span class="math inline">\(v&#39;(u_b)=\Phi(v(u_b))\)</span>映射得到<span class="math inline">\(v&#39;(u_b)\)</span>，然后再映射回来通过<span class="math inline">\(\Phi^{-1}(v&#39;(u_b))\)</span>来得到映射<span class="math inline">\(v&#39;&#39;(u_b)\)</span>。请注意，在这个无监督的学习过程中不需要标签，因此标记锚节点和未标记锚节点之间没有差异。这个自动映射的损失是基于<span class="math inline">\(v(u_b)\)</span>和<span class="math inline">\(v&#39;&#39;(u_b)\)</span>之间的差值来计算的。我们使用同样的方法对Gt→Gs模型进行预训练。在这个无监督的预训练之后，我们有两个弱映射函数Φ和<span class="math inline">\(Φ^{-1}\)</span>，它们将在下一步中得到进一步的改进。</p><ul><li>有监督的UIL学习</li></ul><p>利用标记的锚定节点，通过玩双重学习游戏来改进映射函数Φ和<span class="math inline">\(Φ^{-1}\)</span>。具体的，假设h个batches和n个anchor nodes，其中每个batch有<span class="math inline">\(\lfloor n/h \rfloor\)</span>个标记的节点。每个batch组成了一个episode，其中一个anchor node <span class="math inline">\(u_a\)</span>表示一个状态<span class="math inline">\(s_a\)</span>。请注意，在这种情况下的状态转换是确定性的，当前状态的概率为1转移到下一个状态（锚节点），该操作被定义为选择一个锚节点。我们使用<span class="math inline">\(v(u_a)\)</span>和<span class="math inline">\(v&#39;(u_a)\)</span>分别表示<span class="math inline">\(u_a\)</span>在网络<span class="math inline">\(G^s\)</span>和网络<span class="math inline">\(G^t\)</span>中的向量表示。给定一个batch的anchor nodes，两个mapping func需要尽力根据批处理中映射锚定节点的奖励，对齐两个用户潜在空间。</p><p>对于一个从在<span class="math inline">\(G^s\)</span>中的<span class="math inline">\(u_a\)</span>开始的游戏。我们使用<span class="math inline">\(\Phi\)</span>来映射它的向量到<span class="math inline">\(G^t\)</span>的空间中，并且搜索其k个最邻近的向量<span class="math inline">\(S(v&#39;(u_a))=Top(\Phi(v(u_a)))\)</span>，包含Gt中锚定节点最相似的k个嵌入向量。这里，k个向量是真实用户的候选，在更多锚节点上训练，成功链接的概率更高。在<span class="math inline">\(G^t\)</span>中的agent B随后可以计算一个奖励<span class="math inline">\(r^i_{s,t}\)</span>：</p><p><img src="/2022/05/27/DeepLink/奖励.png"></p><p>寻找并且平均topk的<span class="math inline">\(\Phi(v(u_a))\)</span>的原因在于映射函数很难精确地匹配<span class="math inline">\(u_a∈G^t\)</span>(向量<span class="math inline">\(v&#39;(u_a)\)</span>）的实恒等式，然而，它有一个更大的概率在前k最近的k中包含真实的恒等式。</p><p>直观的，我们也可以计算<span class="math inline">\(v&#39;(u_i)\)</span>的映射返回<span class="math inline">\(G^s\)</span>，并且利用映射的二重性来产生第二个奖励<span class="math inline">\(r^a_{(t,s)}\)</span>也就是<span class="math inline">\(\Phi^{-1}(v&#39;(u_i))\)</span>与<span class="math inline">\(v(u_a)\)</span>之间的平均相似度。</p><p><img src="/2022/05/27/DeepLink/奖励1.png"></p><p>因此，动作值<span class="math inline">\(r^a\)</span>对于一个选定的用户<span class="math inline">\(u_a\)</span>来说就是<span class="math inline">\(r^a_{s,t}, r^a_{t,s}\)</span>的线性组合，表明映射函数正确实身份链接的估计概率。特别是，它利用两个映射函数的对偶性来指导锚定节点的训练过程。期望的奖励<span class="math inline">\(E[r_h]\)</span>在第h轮的batch是：</p><p><img src="/2022/05/27/DeepLink/奖励期望.png"></p><p>由于这个游戏的奖励可以被认为是<span class="math inline">\(v&#39;(u_a),v(u_a),\Phi,\Phi^{-1}\)</span>的函数，我们可以在最大化期望奖励的两个映射函数中优化参数W和b，其中<span class="math inline">\(\gamma^h_{s,t}\)</span>以及<span class="math inline">\(\gamma^h_{t,s}\)</span>是使用政策梯度法的贴现率。</p><p>我们也使用同样的方法从另一个方向训练深度Gs→Gt，以缓解过拟合。根据经验，我们发现平均连锁结果是有帮助对齐的 有效地连接两个网络。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;deeplink&quot;&gt;DeepLink&lt;/h1&gt;
&lt;p&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="论文笔记" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Graph Embedding</title>
    <link href="http://yoursite.com/2022/05/25/Graph-Embedding/"/>
    <id>http://yoursite.com/2022/05/25/Graph-Embedding/</id>
    <published>2022-05-24T16:02:53.000Z</published>
    <updated>2022-05-30T02:06:18.468Z</updated>
    
    <content type="html"><![CDATA[<h1 id="networkx库介绍">1. networkx库介绍</h1><p><a id="more"></a></p><p>&lt;! -- more --&gt;</p><h2 id="图数据结构">1.1 图数据结构</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx</span><br><span class="line">G = nx.Graph()</span><br></pre></td></tr></table></figure><p>这里定义的图G是一组节点（顶点）和已经识别的节点对（边）的集合。节点可以是任意的可哈希的对象，例如文本字符串、图像等。图的初始化的源码解析如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""Initialize a graph with edges, name, or graph attributes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        incoming_graph_data : input graph (optional, default: None)</span></span><br><span class="line"><span class="string">            Data to initialize graph. If None (default) an empty</span></span><br><span class="line"><span class="string">            graph is created.  The data can be an edge list, or any</span></span><br><span class="line"><span class="string">            NetworkX graph object.  If the corresponding optional Python</span></span><br><span class="line"><span class="string">            packages are installed the data can also be a NumPy matrix</span></span><br><span class="line"><span class="string">            or 2d ndarray, a SciPy sparse matrix, or a PyGraphviz graph.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        attr : keyword arguments, optional (default= no attributes)</span></span><br><span class="line"><span class="string">            Attributes to add to graph as key=value pairs.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        See Also</span></span><br><span class="line"><span class="string">        --------</span></span><br><span class="line"><span class="string">        convert</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Examples</span></span><br><span class="line"><span class="string">        --------</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; G = nx.Graph()  # or DiGraph, MultiGraph, MultiDiGraph, etc</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; G = nx.Graph(name='my graph')</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; e = [(1, 2), (2, 3), (3, 4)]  # list of edges</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; G = nx.Graph(e)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Arbitrary graph attribute pairs (key=value) may be assigned</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; G = nx.Graph(e, day="Friday")</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; G.graph</span></span><br><span class="line"><span class="string">        &#123;'day': 'Friday'&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br></pre></td></tr></table></figure><h2 id="从文件中导入图数据">1.2 从文件中导入图数据</h2><p>一般使用最多的函数是read_edgelist,作用是从一个边的集合中来获取一幅图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_edgelist</span><span class="params">(path, comments=<span class="string">"#"</span>, delimiter=None, create_using=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                  nodetype=None, data=True, edgetype=None, encoding=<span class="string">'utf-8'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Read a graph from a list of edges.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    path : file or string</span></span><br><span class="line"><span class="string">       File or filename to read. If a file is provided, it must be</span></span><br><span class="line"><span class="string">       opened in 'rb' mode.</span></span><br><span class="line"><span class="string">       Filenames ending in .gz or .bz2 will be uncompressed.</span></span><br><span class="line"><span class="string">    comments : string, optional</span></span><br><span class="line"><span class="string">       The character used to indicate the start of a comment.</span></span><br><span class="line"><span class="string">    delimiter : string, optional</span></span><br><span class="line"><span class="string">       The string used to separate values.  The default is whitespace.</span></span><br><span class="line"><span class="string">    create_using : NetworkX graph constructor, optional (default=nx.Graph)</span></span><br><span class="line"><span class="string">       Graph type to create. If graph instance, then cleared before populated.</span></span><br><span class="line"><span class="string">    nodetype : int, float, str, Python type, optional</span></span><br><span class="line"><span class="string">       Convert node data from strings to specified type</span></span><br><span class="line"><span class="string">    data : bool or list of (label,type) tuples</span></span><br><span class="line"><span class="string">       Tuples specifying dictionary key names and types for edge data</span></span><br><span class="line"><span class="string">    edgetype : int, float, str, Python type, optional OBSOLETE</span></span><br><span class="line"><span class="string">       Convert edge data from strings to specified type and use as 'weight'</span></span><br><span class="line"><span class="string">    encoding: string, optional</span></span><br><span class="line"><span class="string">       Specify which encoding to use when reading file.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    G : graph</span></span><br><span class="line"><span class="string">       A networkx Graph or other type specified with create_using</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Examples</span></span><br><span class="line"><span class="string">    --------</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; nx.write_edgelist(nx.path_graph(4), "test.edgelist")</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; G=nx.read_edgelist("test.edgelist")</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; fh=open("test.edgelist", 'rb')</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; G=nx.read_edgelist(fh)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; fh.close()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; G=nx.read_edgelist("test.edgelist", nodetype=int)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; G=nx.read_edgelist("test.edgelist",create_using=nx.DiGraph)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Edgelist with data in a list:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; textline = '1 2 3'</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; fh = open('test.edgelist','w')</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; d = fh.write(textline)</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; fh.close()</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; G = nx.read_edgelist('test.edgelist', nodetype=int, data=(('weight',float),))</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; list(G)</span></span><br><span class="line"><span class="string">    [1, 2]</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; list(G.edges(data=True))</span></span><br><span class="line"><span class="string">    [(1, 2, &#123;'weight': 3.0&#125;)]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    See parse_edgelist() for more examples of formatting.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    See Also</span></span><br><span class="line"><span class="string">    --------</span></span><br><span class="line"><span class="string">    parse_edgelist</span></span><br><span class="line"><span class="string">    write_edgelist</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Notes</span></span><br><span class="line"><span class="string">    -----</span></span><br><span class="line"><span class="string">    Since nodes must be hashable, the function nodetype must return hashable</span></span><br><span class="line"><span class="string">    types (e.g. int, float, str, frozenset - or tuples of those, etc.)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''以下是调用'''</span></span><br><span class="line">self.G = nx.read_edgelist(filename,create_using=nx.DiGraph,nodetype=int)</span><br></pre></td></tr></table></figure><h1 id="网络嵌入方法实现">2. 网络嵌入方法实现</h1><h2 id="deepwalk">2.1 deepwalk</h2><p>随机游走突出一个字随机，以下是该模型的主要算法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_sequence</span><span class="params">(self, start_node)</span> -&gt; List[int]:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    -&gt;用于类型提醒，返回的是列表</span></span><br><span class="line"><span class="string">    对单个节点进行随机游走算法</span></span><br><span class="line"><span class="string">    :param start_node:</span></span><br><span class="line"><span class="string">    :return: 一条随机游走序列</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    walk = [start_node]<span class="comment">#放的是节点的id</span></span><br><span class="line">    <span class="string">'''在邻居中取walk_length的节点，少了的话就取所有'''</span></span><br><span class="line">    <span class="keyword">while</span> len(walk) &lt; self.walk_length:</span><br><span class="line">        current_node = walk[<span class="number">-1</span>]</span><br><span class="line">        neighbors = list(self.g.neighbors(current_node))</span><br><span class="line">        <span class="keyword">if</span> len(neighbors) &gt; <span class="number">0</span>:</span><br><span class="line">            walk.append(random.choice(neighbors))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> walk</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_walk</span><span class="params">(self)</span> -&gt; List[List[int]]:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    随机游走算法，采集图中点的序列，超参数walk_length,window_size,大规模图可以并行优化</span></span><br><span class="line"><span class="string">    :return: 二维list，随机游走生成的序列</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    walks = []<span class="comment">#存放的是整个随机游走序列</span></span><br><span class="line">    <span class="keyword">for</span> walk_iter <span class="keyword">in</span> range(self.num_walks):</span><br><span class="line">        print(<span class="string">"当前游走次数：&#123;&#125;"</span>.format(walk_iter + <span class="number">1</span>))</span><br><span class="line">        node_list = list(self.g.nodes())</span><br><span class="line">        random.shuffle(node_list)<span class="comment">#随机打乱节点</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> node_list:</span><br><span class="line">            walks.append(self.get_sequence(node))</span><br><span class="line">    <span class="keyword">return</span> walks</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self)</span>:</span></span><br><span class="line">    t = time.time()</span><br><span class="line">    print(<span class="string">"开始随机游走"</span>)</span><br><span class="line">    sentence = self.random_walk()</span><br><span class="line">    corpus = []</span><br><span class="line">    <span class="comment">#将sentence转换为str类型</span></span><br><span class="line">    <span class="keyword">for</span> idx, each <span class="keyword">in</span> enumerate(sentence):</span><br><span class="line">        corpus.append(list(map(str, sentence[idx])))</span><br><span class="line">    print(<span class="string">"随机游走结束，开始训练！"</span>)</span><br><span class="line">    word2vec = Word2Vec(</span><br><span class="line">        sentences=corpus,</span><br><span class="line">        vector_size=self.dim,</span><br><span class="line">        window=self.window_size,</span><br><span class="line">        min_count=<span class="number">0</span>,</span><br><span class="line">        workers=<span class="number">0</span>,</span><br><span class="line">        sg=<span class="number">1</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> self.g.nodes():</span><br><span class="line">        self.vectors[node] = word2vec.wv[str(node)]</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"模型训练完成！算法总共消耗时间：&#123;&#125;秒"</span>.format(round(time.time() - t, <span class="number">3</span>)))</span><br></pre></td></tr></table></figure><h2 id="node2vec">2.2 node2vec</h2>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;networkx库介绍&quot;&gt;1. networkx库介绍&lt;/h1&gt;
&lt;p&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="研究工作" scheme="http://yoursite.com/tags/%E7%A0%94%E7%A9%B6%E5%B7%A5%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>java基础补充</title>
    <link href="http://yoursite.com/2022/05/22/java%E5%9F%BA%E7%A1%80%E8%A1%A5%E5%85%85/"/>
    <id>http://yoursite.com/2022/05/22/java%E5%9F%BA%E7%A1%80%E8%A1%A5%E5%85%85/</id>
    <published>2022-05-22T13:11:47.000Z</published>
    <updated>2022-05-30T04:14:13.155Z</updated>
    
    <content type="html"><![CDATA[]]></content>
    
    <summary type="html">
    
      
      
        

      
    
    </summary>
    
    
    
      <category term="java基础知识" scheme="http://yoursite.com/tags/java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>算法笔记</title>
    <link href="http://yoursite.com/2022/05/16/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2022/05/16/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/</id>
    <published>2022-05-16T14:01:52.000Z</published>
    <updated>2022-05-30T02:06:07.272Z</updated>
    
    <content type="html"><![CDATA[<h1 id="排序">2. 排序</h1><p><a id="more"></a></p><h2 id="选择排序">2.1 选择排序</h2><p>选择排序的思想是：首先找到数组中最小的那个元素，其次，将它和数组中的第一个元素交换位置，然后再在剩下的元素中找到最小的元素，将它与第二个元素进行交换，如此往复，直到整个数组都是有序的。</p><h2 id="冒泡排序">2.2 冒泡排序</h2><p>冒泡排序的思想是：比较列表的相邻的数，如果前面的比后面的要大，那么就交换这两个，一趟排序完成之后，则无序区减少一个数，有序区增加一个数。</p><h2 id="插入排序">2.3 插入排序</h2><p>插入排序是一种最简单的排序方法，它的基本思想就是将一个记录插入到已经排好序的有序列表中，从而一个新的、记录增加1的有序表。</p><h2 id="希尔排序">2.4 希尔排序</h2><p>希尔排序思想来自插入排序，只不过不像插入排序那样只改变相邻的元素，而是交换不相邻的元素。希尔排序的思想是使数组中任意间隔为h的元素都是有序的。这样的数组编织在一起组成的一个数组，如果h很大，我们就能够将元素移动到很远的地方，为实现更小的h有序创建方便。</p><p>简单来讲，分组排序就是首先以一定的间隔（通常是数组的一半长度），对组内的元素进行排序，然后不断减半这个间隔h，直到1.</p><h2 id="快速排序">2.5 快速排序</h2><p>快速排序是对冒泡排序的一种改进，通过多次比较和交换来实现排序，流程如下：</p><p>（1）首先设定一个分界值，通过该分界值将数组分为左右两部分。</p><p>（2）将大于或等于分界值的数据集中到数组右边，小于分界值的数值集中到数组的左边。此时，左边部分各个元素都小于分界值，右边元素都大于等于分界值。</p><p>（3）然后，左边和右边的数据可以独立排序，对于左侧的数据，又可以取一个分界值，将该部分数据分为左右两部分，同样在左边放较小的值，右边放置较大的值。右侧的数据可以做类似的处理。</p><p>（4）重复上述步骤，可以看出这是个递归的定义。通过递归将左侧部分排好序之后，再递归排好右侧部分的顺序，当左右两个部分的数据排序完成之后，整个数组排序完成。</p><h2 id="堆排序">2.6 堆排序</h2><p>首先堆是一个完全近似二叉树的结构，并且同时满足堆积的性质，即子结点的键值或者索引总是小于或者大于它的父节点。</p><p>算法流程：</p><ul><li>将无序序列构建成一个堆，根据升序降序需求选择大顶堆或者小顶堆，大顶堆是节点大于叶子结点的数据，大顶堆用于升序排序</li><li>将堆顶元素与末尾元素交换，将最大元素沉到数组末尾</li><li>重新调整结构，使其满足堆的定义，然后继续交换堆顶元素与当前末尾元素，反复执行调整+交换步骤，直到整个序列有序。</li></ul><h2 id="归并排序">2.7 归并排序</h2><p>归并算法的思想是：采用分治的思想，将已经有序的两个子序列合并得到完全有序的序列，即先使每个子序列有序，再使子序列段间有序。（1）把长度为n的输入序列分为两个长度为n/2的子序列；（2）对这两个子序列分别采用归并排序；（3）将两个排好序的子序列合并成一个最终的排序序列</p><h2 id="桶排序">2.8 桶排序</h2><p>其实基数排序和计数排序都是基于桶排序的思想发展来的。工作原理是将数组分到有限数量的桶子里面，每个桶再分别进行排序（使用别的算法或者递归继续桶排序）。若待排序的记录的关键字在一个明显有限的范围内时，可以设计有限个有序桶，每个桶只能装预置对应的值，顺序输出各桶的值，将得到有序的序列。简单来说，在我们可以确定需要排列的数组范围时，可以生成该数值范围内有限个桶去对应数组中的数，然后我们将扫描的数值放入匹配的桶里的行为，可以看做是分类，在分类完成后，我们需要依次按照桶的顺序输出桶内存放的数值，这样就完成了桶排序。</p><h2 id="基数排序">2.9 基数排序</h2><p>桶排序的一种，非比较排序，多关键字排序。基本操作是：先找出数据中心最大的位数，然后从低位开始排序，收集已排序的数据，直到最高位排序完成。</p><p>算法的流程为：（1）先计算出数据的最大位数；（2）先从低位开始排序，再从高位开始排序，收集已经排序的数据；（3）不断从低位到高位，对数据进行排序，直到最高位已经排好，排序完成。</p><h2 id="计数排序">2.10 计数排序</h2><p>假设输入的线性表L的长度为n，L=L1,L2,..,Ln；线性表的元素属于有限偏序集S，|S|=k且k=O(n)，S={S1,S2,..Sk}；则计数排序可以描述如下：</p><p>1、扫描整个集合S，对每一个Si∈S，找到在线性表L中小于等于Si的元素的个数T(Si)；</p><p>2、扫描整个线性表L，对L中的每一个元素Li，将Li放在输出线性表的第T(Li)个位置上，并将T(Li)减1。</p><h2 id="优先队列">2.11 优先队列</h2><p>优先队列是一种抽象数据类型，它表示了一组值和对这些值的操作。优先队列的最重要的方法就是删除最大元素以及插入元素。</p><p>如果我们使用指针来表示堆有序的二叉树，那么每个元素都需要三个指针来找到其上下节点，父节点和两个子结点各需一个。完全二叉树只用数组而不需要指针就可以表示。二叉堆是一组能够能够用有序的完全二叉树排序的元素，并在数组中安照层级存储（不使用数组的第一个位置）。一颗大小为N的完全二叉树的高度为<span class="math inline">\(\lfloor lg N \rfloor\)</span>。</p><p>对于一个含有N个元素的基于堆的优先队列，插入元素操作只需要不超过<span class="math inline">\((lgN+1)\)</span>次比较，删除最大元素的操作只需要不超过<span class="math inline">\(2lgN\)</span>次操作。</p><h1 id="散列表">1. 散列表</h1><p>散列表查找包含两个步骤：（1）用散列函数将被查找的键转化为数组的一个索引；（2）处理一个冲突碰撞的过程。</p><p>（1）对于每一种键都需要一个与之对应的散列函数。</p><ul><li><p>1.正整数：除留余数法。选择大小为素数M的数组，对于任意正整数k，计算k除以M的余数，可以将键有效的散布在0到M-1的范围内。如果M不是素数，我们可能无法利用键中包含的所有信息，导致无法均匀的散列散列值。</p></li><li><p>2.浮点数：如果键是0到1之间的实数，我们可以将它乘以M并四舍五入得到一个0到M-1之间的索引值。这种方法的缺陷是：键的高位起的作用更大，最低位对散列的结果没有影响。修正这个问题的方法是将键表示为二进制数然后再使用除留余数法。</p></li><li><p>3.字符串：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> hash = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; s.length(); i++)&#123;</span><br><span class="line">    hash = (R * hash + s.charAt(i)) % M;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>charAt方法能够返回一个非负16位整数，如果R比任何字符的值都大，这种计算相当于将字符串当做一个N位的R进制值，将它除以M并求余。</p></li><li><p>4.组合键：如果键的类型含有多个整型变量，可以和String类型一样将他们混合起来。如date类型(day,month,year)，可以表示散列值为：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> hash = (((day * R + month) % M) * R + year) % M;</span><br></pre></td></tr></table></figure><p>（2）散列算法的第二步就是碰撞处理</p><ul><li><p>拉链法：将大小为M的数组中的每个元素指向一条链表，链表中的每个节点都存储了散列值为该元素的索引的键值对。查找分为两步：首先根据散列值找到对应的链表，然后沿着链表的熟悉怒查找相应的键。</p></li><li><p>线性探测法：使用大小为M的数组保存N个键值对，其中M &gt; N。需要依靠数组中的空位解决碰撞冲突问题，基于这种方法的方法统称为开放地址散列表。其中最简单的叫做线性探测法，当发生碰撞时，也就是一个键的散列值已经被另一个不同的键占用，直接检测散列表的下一个位置。这样可能产生三种结果：</p><p>（1）命中，该位置的键和被查找的键相同</p><p>（2）未命中，键为空（该位置没有键）</p><p>（3）继续查找，该位置的键和查找的键不同</p><p>用散列函数找到键在数组中的索引，检查其中的键和被查找的键是否相同，，如果不同则继续查找，将索引增大，到达数组结尾时折回数组的开头，直到找到该键或者遇到一个空元素。</p></li></ul><h2 id="稀疏向量">稀疏向量</h2><p>对数据进行处理的时候，一般需要对类别型特征进行编码，比方说独热编码就是一种稀疏向量。对于一个向量<span class="math inline">\(v=(v_1,v_2,...,v_n)\)</span>如果v仅在少量的维度上取值不为0，称之为稀疏向量。由于稀疏向量的非零值比较少，可以通过仅存储非零值的方式来节省空间。每个非零值都可以用一个<span class="math inline">\((index,value)\)</span>对来表示。</p><p>对于一个n维整数向量v，如果其仅在a个维度上取值不为0，则可以惟一的表示为：</p><p><span class="math inline">\([(index1,value1),(index_2,value_2),...,(index_a,value_a)]\)</span></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;排序&quot;&gt;2. 排序&lt;/h1&gt;
&lt;p&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>PERFECT笔记</title>
    <link href="http://yoursite.com/2022/05/06/PERFECT%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2022/05/06/PERFECT%E7%AC%94%E8%AE%B0/</id>
    <published>2022-05-06T10:12:23.000Z</published>
    <updated>2022-05-30T02:06:55.590Z</updated>
    
    <content type="html"><![CDATA[<p><a id="more"></a></p><p>现有的网络对齐的方法主要关注个人用户层面上的对齐，需要具有大量的个人共享的信息。在缺少相关的共享信息的时候，社区的对齐可能会给用户对齐以关键的补充信息。反过来，用户对齐也揭示了更多关于社区对齐的线索。我们引入了联合社交网络对齐问题，目的是跨社交网络同时对齐用户和社区。</p><ul><li>如何同时学习用户和社区的表示方式？</li><li>如何使用户对齐和社区对齐相互受益？</li></ul><p><img src="../images/PERFECT/联合网络对齐.png"></p><p>使用双曲空间和欧式空间来嵌入网络得到：</p><p><img src="../images/PERFECT/嵌入空间对比.png"></p><p>对比欧式空间，双曲空间倾向于呈现输入网络中节点之间的潜在层次结构。我们观察到，该层次特征在社交网络[9]中是常见的，更重要的是，它对用户对齐[2]和有利于社区发现至关重要。</p><p>问题定义：给定一对社交网络<span class="math inline">\(G^s,G^t\)</span>以及anchor set A，要找到所有的anchor users：<span class="math inline">\(\left\{(v_i^s,v_k^t)\right\}\)</span>以及所有的anchor communities：<span class="math inline">\(\left\{(C_p^s,C_q^t)\right\}\)</span>。</p><h2 id="为什么使用双曲空间嵌入">为什么使用双曲空间嵌入</h2><p>与欧几里得空间相比，双曲空间中的嵌入编码了用户之间的潜在层次结构，即中心性较高的用户倾向于居住在更靠近原点的位置。层次特征已被证明对用户对齐[2]和有利于社区发现至关重要。以定量地测量层次特征，引入了$Gromov   -hyperbolicity $，一个几何群论的度量。</p><p>相应的δ−双曲性显示了数据集的潜在层次结构：</p><p><img src="../images/PERFECT/数据集.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="论文笔记" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>刷题指南</title>
    <link href="http://yoursite.com/2022/05/03/%E5%88%B7%E9%A2%98%E6%8C%87%E5%8D%97/"/>
    <id>http://yoursite.com/2022/05/03/%E5%88%B7%E9%A2%98%E6%8C%87%E5%8D%97/</id>
    <published>2022-05-02T17:53:58.000Z</published>
    <updated>2022-05-30T02:07:09.482Z</updated>
    
    <content type="html"><![CDATA[<h1 id="poj刷题顺序">1. POJ刷题顺序</h1><p><a id="more"></a></p><h1 id="初期">初期:</h1><h2 id="一.基本算法">一.基本算法:</h2><p>枚举. (POJ 1753,POJ 2965) 贪心(POJ 1328,POJ 2109,POJ 2586) 递归和分治法. 递推. 构造法.(POJ 3295) 模拟法.(POJ 1068,POJ 2632,POJ 1573,POJ 2993,POJ 2996)</p><h2 id="二.图算法">二.图算法:</h2><p>图的深度优先遍历和广度优先遍历. 最短路径算法(dijkstra,bellman-ford,floyd,heap+dijkstra) (POJ 1860,POJ 3259,POJ 1062,POJ 2253,POJ 1125,POJ 2240) 最小生成树算法(prim,kruskal) (POJ 1789,POJ 2485,POJ 1258,POJ 3026) 拓扑排序 (POJ 1094) 二分图的最大匹配 (匈牙利算法) (POJ 3041,POJ 3020) 最大流的增广路算法(KM算法). (POJ 1459,POJ 3436)</p><h2 id="三.数据结构.">三.数据结构.</h2><p>串 (POJ 1035,POJ 3080,POJ 1936) 排序(快排、归并排(与逆序数有关)、堆排) (POJ 2388,POJ 2299) 简单并查集的应用. 哈希表和二分查找等高效查找法(数的Hash,串的Hash) (POJ 3349,POJ 3274,POJ 2151,POJ 1840,POJ 2002,POJ 2503) 哈夫曼树(POJ 3253) 堆 trie树(静态建树、动态建树) (POJ 2513)</p><h2 id="四.简单搜索">四.简单搜索</h2><p>深度优先搜索 (POJ 2488,POJ 3083,POJ 3009,POJ 1321,POJ 2251) 广度优先搜索(POJ 3278,POJ 1426,POJ 3126,POJ 3087.POJ 3414) 简单搜索技巧和剪枝(POJ 2531,POJ 1416,POJ 2676,POJ 1129)</p><h2 id="五.动态规划">五.动态规划</h2><p>背包问题. (POJ 1837,POJ 1276) 型如下表的简单DP(可参考lrj的书 page149): E[j]=opt{D+w(i,j)} (POJ 3267,POJ 1836,POJ 1260,POJ 2533) E[i,j]=opt{D[i-1,j]+xi,D[i,j-1]+yj,D[i-1][j-1]+zij} (最长公共子序列) (POJ 3176,POJ 1080,POJ 1159) C[i,j]=w[i,j]+opt{C[i,k-1]+C[k,j]}.(最优二分检索树问题)</p><h2 id="六.数学">六.数学</h2><p>组合数学:</p><p>加法原理和乘法原理. 排列组合. 递推关系. (POJ 3252,POJ 1850,POJ 1019,POJ 1942) 数论.</p><p>素数与整除问题 进制位. 同余模运算. (POJ 2635, POJ 3292,POJ 1845,POJ 2115) 计算方法.</p><p>二分法求解单调函数相关知识.(POJ 3273,POJ 3258,POJ 1905,POJ 3122)</p><h2 id="七.计算几何学.">七.计算几何学.</h2><p>几何公式. 叉积和点积的运用(如线段相交的判定,点到线段的距离等). (POJ 2031,POJ 1039) 多边型的简单算法(求面积)和相关判定(点在多边型内,多边型是否相交) (POJ 1408,POJ 1584) 凸包. (POJ 2187,POJ 1113)</p><h1 id="中级">中级:</h1><h2 id="一.基本算法-1">一.基本算法:</h2><p>C++的标准模版库的应用. (POJ 3096,POJ 3007) 较为复杂的模拟题的训练(POJ 3393,POJ 1472,POJ 3371,POJ 1027,POJ 2706)</p><h2 id="二.图算法-1">二.图算法:</h2><p>差分约束系统的建立和求解. (POJ 1201,POJ 2983) 最小费用最大流(POJ 2516,POJ 2195) 双连通分量(POJ 2942) 强连通分支及其缩点.(POJ 2186) 图的割边和割点(POJ 3352) 最小割模型、网络流规约(POJ 3308)</p><h2 id="三.数据结构.-1">三.数据结构.</h2><p>线段树. (POJ 2528,POJ 2828,POJ 2777,POJ 2886,POJ 2750) 静态二叉检索树. (POJ 2482,POJ 2352) 树状树组(POJ 1195,POJ 3321) RMQ. (POJ 3264,POJ 3368) 并查集的高级应用. (POJ 1703,POJ 2492) KMP算法. (POJ 1961,POJ 2406)</p><h2 id="四.搜索">四.搜索</h2><p>最优化剪枝和可行性剪枝 搜索的技巧和优化 (POJ 3411,POJ 1724) 记忆化搜索(POJ 3373,POJ 1691)</p><h2 id="五.动态规划-1">五.动态规划</h2><p>较为复杂的动态规划(如动态规划解特别的施行商问题等) (POJ 1191,POJ 1054,POJ 3280,POJ 2029,POJ 2948,POJ 1925,POJ 3034) 记录状态的动态规划. (POJ 3254,POJ 2411,POJ 1185) 树型动态规划(POJ 2057,POJ 1947,POJ 2486,POJ 3140)</p><h2 id="六.数学-1">六.数学</h2><p>组合数学:</p><p>容斥原理. 抽屉原理. 置换群与Polya定理(POJ 1286,POJ 2409,POJ 3270,POJ 1026). 递推关系和母函数. 数学.</p><p>高斯消元法(POJ 2947,POJ 1487,POJ 2065,POJ 1166,POJ 1222) 概率问题. (POJ 3071,POJ 3440) GCD、扩展的欧几里德(中国剩余定理) (POJ 3101) 计算方法.</p><p>0/1分数规划. (POJ 2976) 三分法求解单峰(单谷)的极值. 矩阵法(POJ 3150,POJ 3422,POJ 3070) 迭代逼近(POJ 3301) 随机化算法(POJ 3318,POJ 2454)</p><p>杂题. (POJ 1870,POJ 3296,POJ 3286,POJ 1095)</p><h2 id="七.计算几何学.-1">七.计算几何学.</h2><p>坐标离散化. 扫描线算法(例如求矩形的面积和周长并,常和线段树或堆一起使用). (POJ 1765,POJ 1177,POJ 1151,POJ 3277,POJ 2280,POJ 3004) 多边形的内核(半平面交)(POJ 3130,POJ 3335) 几何工具的综合应用.(POJ 1819,POJ 1066,POJ 2043,POJ 3227,POJ 2165,POJ 3429)</p><h1 id="高级">高级:</h1><h2 id="一.基本算法要求">一.基本算法要求:</h2><p>代码快速写成,精简但不失风格 (POJ 2525,POJ 1684,POJ 1421,POJ 1048,POJ 2050,POJ 3306) 保证正确性和高效性. POJ 3434</p><h2 id="二.图算法-2">二.图算法:</h2><p>度限制最小生成树和第K最短路. (POJ 1639) 最短路,最小生成树,二分图,最大流问题的相关理论(主要是模型建立和求解) (POJ 3155, POJ 2112,POJ 1966,POJ 3281,POJ 1087,POJ 2289,POJ 3216,POJ 2446 ) 最优比率生成树. (POJ 2728) 最小树形图(POJ 3164) 次小生成树. 无向图、有向图的最小环</p><h2 id="三.数据结构.-2">三.数据结构.</h2><p>trie图的建立和应用. (POJ 2778) LCA和RMQ问题(LCA(最近公共祖先问题) 有离线算法(并查集+dfs) 和 在线算法 (RMQ+dfs)).(POJ 1330) 双端队列和它的应用(维护一个单调的队列,常常在动态规划中起到优化状态转移 的目的). (POJ 2823) 左偏树(可合并堆). 后缀树(非常有用的数据结构,也是赛区考题的热点). (POJ 3415,POJ 3294)</p><h2 id="四.搜索-1">四.搜索</h2><p>较麻烦的搜索题目训练(POJ 1069,POJ 3322,POJ 1475,POJ 1924,POJ 2049,POJ 3426) 广搜的状态优化:利用M进制数存储状态、转化为串用hash表判重、按位压缩存储 状态、双向广搜、A<em>算法. (POJ 1768,POJ 1184,POJ 1872,POJ 1324,POJ 2046,POJ 1482) 深搜的优化:尽量用位运算、一定要加剪枝、函数参数尽可能少、层数不易过大 、可以考虑双向搜索或者是轮换搜索、IDA</em>算法. (POJ 3131,POJ 2870,POJ 2286)</p><h2 id="五.动态规划-2">五.动态规划</h2><p>需要用数据结构优化的动态规划. (POJ 2754,POJ 3378,POJ 3017) 四边形不等式理论. 较难的状态DP(POJ 3133)</p><h2 id="六.数学-2">六.数学</h2><p>组合数学.</p><p>MoBius反演(POJ 2888,POJ 2154) 偏序关系理论. 博奕论.</p><p>极大极小过程(POJ 3317,POJ 1085) Nim问题.</p><h2 id="七.计算几何学.-2">七.计算几何学.</h2><p>半平面求交(POJ 3384,POJ 2540) 可视图的建立(POJ 2966) 点集最小圆覆盖. 对踵点(POJ 2079)</p><h2 id="八.综合题.">八.综合题.</h2><p>(POJ 3109,POJ 1478,POJ 1462,POJ 2729,POJ 2048,POJ 3336,POJ 3315,POJ 2148,POJ 1263</p><h1 id="hdu刷题指南">2. HDU刷题指南</h1><p>第一阶段：开始入门吧！（15天，53题） 一．输入输出练习（2天，10题） 1000、1089—1096、1001 二．简单操作：（2—4天，12题） 2000—2011、2039 三．英文题试水（3—4天，8题） 1720、1062、2104、1064、2734、1170、1197、2629 四．回归水题（4-6天，24题） 2012—2030、2032、2040、2042、2054、2055 （第一阶段大体结束之后，会由几位学长讲一些数据结构的知识，请同学们务必跟上进度！）</p><p>第二阶段：我要学算法！（12天，31题） 一．字符串我要会处理（2天，6题） 2072、2081、2093、2091、1004、2057 二．简单数学题（4天，12题） 2031、2033、2070、2071、2075、2089、2090、2092、2096—2099 三．要玩就玩汉诺塔（2天,5题） 1995、1996、2064、2077、2175 四．As easy as math（4天，8题） 1108、2138、1713、1722、2136、2504、1717、1125</p><p>第三阶段：acm无底洞啊！（10天，18题） 一．初见dp（2—4天，4题） 2062、1087、1203、1003 二．迷宫之烟雾缭绕（2—4天，3题） 1728、1010、1072 三．数学题做不下去了。（3-5天，8题） 1052、1568、1443、1222、1249、1005、2674、1018 四．龙门客栈，暗藏玄机（2—3天，3题） 1022、1237、1082</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;poj刷题顺序&quot;&gt;1. POJ刷题顺序&lt;/h1&gt;
&lt;p&gt;&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Unsupervised Graph Alignment Alignment with Wasserstein Distance Disciminator笔记</title>
    <link href="http://yoursite.com/2022/04/26/Unsupervised-Graph-Alignment-Alignment-with-Wasserstein-Distance-Disciminator%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2022/04/26/Unsupervised-Graph-Alignment-Alignment-with-Wasserstein-Distance-Disciminator%E7%AC%94%E8%AE%B0/</id>
    <published>2022-04-26T15:36:05.000Z</published>
    <updated>2022-05-30T02:07:18.368Z</updated>
    
    <content type="html"><![CDATA[<p><a id="more"></a></p><p>这篇论文也是最近研究的生成对抗网络解决网络对齐的方法。</p><p>大多数现有的无监督方法都假设相应的节点应该具有相似的局部结构，但这往往不成立。同时，丰富的节点属性往往是可用的，并已被证明可以有效地减轻上述局部拓扑不一致的问题。我们首先开发了一个轻量级的GCN体系结构来捕获局部和全局图模式及其与节点属性的固有相关性。然后证明了在嵌入空间中，得到的最优对齐结果等价于最小化不同图中节点嵌入之间的Wasserstein距离。为此，我们提出了一种新的Wasserstein距离鉴别器来识别候选节点对应对，以更新节点嵌入。</p><p>现在的方法大多数都是基于有监督学习的网络对齐，而现在的大多数的无监督学习任务都是基于结点应该有相似的邻域特性，在实际中太为严格。例如：在不同的图表中，节点的程度可能会迅速变化，用户在Facebook上可能有很多朋友，但在推特上的活跃程度可能会少得多。</p><p>同时，在许多现实世界的图中，节点通常与丰富的侧边信息相关联(也就是节点属性)。这些属性信息与图的结构高度相关，可以用来解决上述局部结构不一致的问题。</p><p>一个简单的解决方案是首先学习节点嵌入，然后找到与学习到的嵌入表示的节点对应关系。这种方法有如下问题：（1）如果我们直接应用GCN来获得节点嵌入，那么不同图的节点嵌入可能不在同一个特征空间中，这不适合于对齐任务。同时，GCN还可能存在过平滑问题，所以就不能有效的捕获全局的图的特征，不适用于对齐任务。（2）将对齐问题简化为基于节点嵌入的相似性矩阵的二部图匹配问题。现在的大多数的算法的复杂度是<span class="math inline">\(O(n^3)\)</span>，很高。（3）节点表示学习和图对齐不是两个独立的任务，而是应该相互补充。更好的节点嵌入可以帮助实现更好的对齐结果，而更好的对齐也可以为嵌入学习提供监督信号。因此，必须在统一的框架。</p><p>这个论文主要回答的问题如下：如何学习适合于对齐任务的有效嵌入内容？如何联合建模嵌入学习和图形对齐？</p><p><span class="math inline">\(G=(v,\varepsilon,X)\)</span>，其中<span class="math inline">\(v=\left\{v_1,v_2,...,v_n\right\}\)</span>，<span class="math inline">\(\xi\)</span>表示边集合，<span class="math inline">\(X∈R^{n \times d}\)</span>表示节点属性矩阵。A表示邻接矩阵。其中这两个网络的节点数量以及特征数量为<span class="math inline">\((n_s,d_s)\)</span>和<span class="math inline">\((n_t,d_t)\)</span></p><ul><li>定义1：（unsupervised graph alignment）给定两个输入的图<span class="math inline">\(G_s=(v_s,\xi_s,X_s)\)</span>以及<span class="math inline">\(G_t=(u_t,\xi_t,X_t)\)</span>，无监督的网络对齐是为了找到节点对<span class="math inline">\(M=\left\{(v_i,u_j)|v_i∈v_S \and u_j ∈ u_t\right\}\)</span>。此外，每个节点最多应该在一个合法的对齐中出现一次。形式上，对于一个合法的M，存在一个部分映射<span class="math inline">\(\pi: v_s \to u_t\)</span>并且一个部分映射<span class="math inline">\(\pi&#39; :u_t \to v_s\)</span>，因此对于任意的<span class="math inline">\((v_i,u_j)∈M\)</span>，我们有<span class="math inline">\(\pi(v_i)=u_j \and \pi&#39;(u_j)=v_i\)</span>。</li></ul><p>我们通常假设<span class="math inline">\(H^{(0)}=X\)</span>，一个两层的GCN架构为：<span class="math inline">\(Z=\widehat A \sigma(\widehat AXW^{(1)})W^{(2)}\)</span>,其中Z是所有的节点的embedding矩阵。给定节点嵌入表示，无监督图对齐问题可以转换为图论中的经典任务，即二部图匹配。</p><ul><li><p>定义2：（network embedding based graph alignment）给定一个节点<span class="math inline">\(v_i\)</span>以及<span class="math inline">\(v_j\)</span>在两个网络中，节点的嵌入为<span class="math inline">\(z_{v_i},z_{u_j}\)</span>，网络对齐的任务是找到：</p><p><img src="../images/wasserstein距离对齐论文/网络对齐任务.png"></p></li></ul><p>如果我们令<span class="math inline">\(c_{i,j}=||z_{v_i}-z_{u_j}||\)</span>,上述的问题等价于节点集为<span class="math inline">\(v_s \or u_t\)</span>以及cost集合<span class="math inline">\(\left\{c_{i,j}|v_i∈v_s,u_j∈u_t\right\}\)</span>的最小化二部图匹配问题。如果是仅使用普通的GCN可能造成的问题有：（1）两个网络的嵌入可能是在不同的空间并且图的全局特征可能不能被很好地刻画；（2）传统的二部图匹配的算法的计算量太大了；（3）节点嵌入可能不适用于对齐任务。</p><p><img src="../images/wasserstein距离对齐论文/模型框架.png"></p><p>传统的GCN只能获取局部的图结构信息，因为第k层的GCN只使用了k阶邻域的信息，随着k的增大，所有节点的embedding将会趋近于一个相似的值。与此同时，全局拓扑模式对于保证准确的对齐也至关重要，特别是当局部结构一致性假设[43]不成立时。最近的研究表明GCN层之间的非线性不是至关重要的，可以简单地去除而不损害学习性能。因此提出了一个轻量级的GCN结构：LGCN，我们移除非线性函数和连接每一层的嵌入：</p><p><img src="../images/wasserstein距离对齐论文/LGCN公式.png"></p><p>由于LGCN避免了层堆叠，因此我们可以将𝐾指定为大量的𝐾来捕获全局拓扑模式。</p><p>由于上述公式除了权重项都能预先计算，稍后将介绍的对抗性训练阶段的计算成本。LGCN的平面结构使模型更容易训练，收敛速度也更快。此外，少量的模型参数将使后期的对抗性训练过程更加稳定。</p><p>考虑到不同的图的属性（包括图的拓扑结构和节点属性）可能会有很大的不同，我们引入了一个额外的变换矩阵T来适应不同图之间的内在差异。<span class="math inline">\(Z_S=Tf_e(A_s,X_s) \ and \ Z_t=f_e(A_t,X_t)\)</span></p><h1 id="wasserstein-distance-discriminator"><strong>Wasserstein Distance Discriminator</strong></h1><p>Wasserstein Distance它是一种广泛用于度量两种概率分布之间差异的度量方法。</p><ul><li>定义3：（Wasserstein Distance）</li></ul><p><img src="../images/wasserstein距离对齐论文/wasserstein距离定义.png"></p><p>𝛾表示一个最优的“质量”运输，它指定了对于每个𝑥，有多少“质量”应该被转移到𝑦，这样P𝑠将变成<span class="math inline">\(P_t\)</span></p><p>从简单的情况出发，令<span class="math inline">\(m=|v_s|=|u_t|\)</span>，我们假设一个完美的从<span class="math inline">\(v_s \to u_t\)</span>的对齐是存在的。它被定义为将V𝑠中的每个节点根据定义2中的最优对应集M与U𝑡中的另一个节点进行匹配的对齐。然后将问题简化为识别给定Z𝑠和Z𝑡的最优集M。我们要最小化在M中的每个节点对<span class="math inline">\((v_i,u_j)\)</span>距离。</p><ul><li>proposition 4：假设<span class="math inline">\(m=|v_s|=|u_t|\)</span>。设P𝑠和P𝑡分别为均匀分布在V𝑠和U𝑡的嵌入上的概率分布。随后我们有等式8：</li></ul><p><img src="../images/wasserstein距离对齐论文/wasserstein距离.png"></p><p>证明：我们首先展示了一个对齐M，它唯一地决定了一个从P𝑠到P𝑡的运输𝛾，因为它可以在等式8中以联合分布的形式表示:</p><p><img src="../images/wasserstein距离对齐论文/分布.png"></p><p>由于<span class="math inline">\(W_1(p_s,p_t)\)</span>表示的最小的转移距离，上面的转移距离至少是<span class="math inline">\(W_1(p_s,p_t)\)</span>，也就是等式8要取小于等于号。要证明等式成立，要做的就是夹逼法证明另一个方向的成立。也就是当<span class="math inline">\(W_1(p_s,p_t)\)</span>达到其最优的时候，所有的在最优的转移<span class="math inline">\(\gamma^*\)</span>中的<span class="math inline">\(p(x=v_i,y=u_j)\)</span>要么是1/m要么是0使得它能够一直生成一个对齐M。<span class="math inline">\(\gamma^*\)</span>决定了一个M。</p><p>为了证明，我们将这个问题转化为一个最小成本的网络流问题<span class="math inline">\((G&#39;,U,C)\)</span>，其中<span class="math inline">\(G&#39;=(V&#39;,\varepsilon&#39;)\)</span>，流量函数<span class="math inline">\(U: \varepsilon&#39; \to R\)</span>以及损失函数<span class="math inline">\(C: \varepsilon&#39; \to R\)</span>。流量网络的构造由以下公式定义：</p><p><img src="../images/wasserstein距离对齐论文/定义.png"></p><p>流量为𝑚的流量在G‘中以U的能力运行，如果我们用将V𝑠和U𝑡之间的每对节点的流量乘以1/𝑚，那么流量就唯一地决定了P𝑠和P之间的传输。由于每个容量𝑈（𝑣，𝑢）和总流量𝑚在这个最小成本流问题中是整数，通过约束的完全单模块化(也称为完整性定理[7,29] 最小代价流问题)，它有一个最优解，在每条边上都有整数流。显然，图中流量等于1的边集等价于M，这意味着总是存在一个M，使𝑊1(P𝑠，P𝑡)最小化。</p><p>然后，我们将命题4推广到一个图中的某些节点在另一个图中可能没有相应的节点的一般情况。</p><ul><li>proposition 5：令<span class="math inline">\(m=|M|\)</span>表示的是<span class="math inline">\(v_s,u_t\)</span>之间对应节点的个数，并且<span class="math inline">\(m ≤ min(|v_s|,|u_t|)\)</span>。对于任意的大小为m的子节点集合<span class="math inline">\(v_s&#39;,u_t&#39;\)</span>，令P‘𝑠和P’𝑡分别是均匀分布在V‘𝑠和U’𝑡嵌入上的相应概率分布。我们随后有：</li></ul><p><img src="../images/wasserstein距离对齐论文/定义1.png"></p><p>证明：与proposition 4类似的，我们将这个问题转化为一个最小的网络流问题：</p><p><img src="../images/wasserstein距离对齐论文/proposition5.png"></p><p>根据约束矩阵[29,32]的完全单模块性，它在每条边上都有一个整数流量的最优解。在这个最优流中，V𝑠中的𝑚个节点和U𝑡中的𝑚个节点将被流连接。设V‘𝑠和U’𝑡是由V𝑠和U𝑡方面的流所选择的节点集,那么V‘𝑠和U’𝑡之间的wasserstein距离（由最优运输计划定义）等价于</p><p><img src="../images/wasserstein距离对齐论文/等价的距离.png"></p><p>另一方面，如果我们假设存在一对子集V‘’𝑆和U‘’𝑡具有更小的wasserstein距离，那么一个更好的解决方案，只包括节点之间的流动 V‘’𝑆和U‘’𝑡，其成本小于最优方案，存在于最小成本网络流问题中。这样的假设显然产生了一个矛盾。</p><p>因此，对于任意的<span class="math inline">\(v_s&#39;&#39;\)</span>以及<span class="math inline">\(u_t&#39;&#39;\)</span>满足<span class="math inline">\(|v_s&#39;&#39;|=m,|u_t&#39;&#39;|=m\)</span>，我们有<span class="math inline">\(W_1(P_s&#39;&#39;,P_t&#39;&#39;)≥W_1(P&#39;_s,P&#39;_t)\)</span>，命题5得证。</p><p>命题5表明，使wasserstein损失最小的节点对是最优的节点对应关系。因此，wasserstein距离反映了匹配的n之间的接近性 结果表明，匹配的最优嵌入表示将使wasserstein距离最小。</p><p><img src="../images/wasserstein距离对齐论文/kantorovich%20rubinstein%20duality.png"></p><p>其中<span class="math inline">\(||f_w||\)</span>是<span class="math inline">\(f_w\)</span>的lipschitz形式，这个等式表示了分离P‘𝑠和P’的最优1-Lipschitz函数𝑓𝑤总是存在的，并且最大的分割是<span class="math inline">\(W_1(P&#39;_s,P&#39;_t)\)</span>.我们使用一个神经网络结构来表示𝑓𝑤，在本文中，它被称为wasserstein距离鉴别器。因此，我们计算了两个不同图中每个节点的𝑓𝑤(z𝑣𝑖)和𝑓𝑤(z𝑢𝑗)。然后，V𝑠中最小化𝑓𝑤(z𝑣𝑖)的节点和U𝑡中最大化𝑓𝑤(z𝑢𝑗)的节点将会 是选定的节点对。选择过程的复杂性将从多项式时间降低到线性时间。</p><p>𝑓𝑤被优化以最大化E𝑥∼P‘𝑠[𝑓𝑤（𝑥）]−E𝑦∼P’𝑡[𝑓𝑤（𝑦）]的预期差异。因此，𝑓𝑤使<span class="math inline">\(-L_{we}\)</span>最小化，从而导致以下损失：</p><p><img src="../images/wasserstein距离对齐论文/LWW损失.png"></p><p>最后，整个优化方案可以表述为一个双人博弈。嵌入学习阶段旨在优化嵌入，以最小化V‘𝑠和U’之间的瓦瑟斯坦距离 ，而对鉴别器进行优化，得到一个更好的wasserstein距离下界。简而言之，这两个阶段在对齐任务的最佳嵌入方面相互补充。</p><p>上述方法的一个问题是，对抗性的损失可能会崩溃为一个平凡的情况，即所有的嵌入向量都成为相同的值。在这种情况下，wasserstein的距离就变成了 0，但这肯定不是我们想要的距离。</p><p>为了解决这个问题，我们使用重构损失来确保嵌入仍然有足够的信息来重新创建输入属性。我们将𝑓𝑟𝑠和𝑓𝑟𝑡定义为两个重构神经网络，每个神经网络都将其图的嵌入作为输入，并返回图的属性作为输出。然后我们将以下损失最小化函数：</p><p><img src="../images/wasserstein距离对齐论文/lossrecon.png"></p><p>最后，我们使用等式（17）来更新嵌入网络中的权值矩阵，其中𝛽是一个平衡两个损失函数的超参数：</p><p><img src="../images/wasserstein距离对齐论文/嵌入的损失函数.png"></p><p>综上所述，在训练过程的每次迭代中，我们的框架首先使用嵌入网络LGCN生成节点嵌入。在此之后，我们的框架将更新了维护者的参数 通过最小化等式中的𝐿ww来实现的ein鉴别器 (15).然后，我们的框架使用瓦瑟斯坦鉴别器，根据算法2生成伪节点对应对。最后，在它的最后 通过最小化等式来更新嵌入网络的参数 (17).我们在算法1中总结了我们的框架的详细训练过程。</p><p><img src="../images/wasserstein距离对齐论文/WALIGN算法.png"></p><p><img src="../images/wasserstein距离对齐论文/候选对算法.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="论文笔记" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>UAGA笔记</title>
    <link href="http://yoursite.com/2022/04/25/UAGA%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2022/04/25/UAGA%E7%AC%94%E8%AE%B0/</id>
    <published>2022-04-25T10:43:23.000Z</published>
    <updated>2022-05-30T02:07:27.820Z</updated>
    
    <content type="html"><![CDATA[<p><a id="more"></a></p><p>这篇文章提出了一个UAGA（Unsupervised Adversarial Graph Alignment）架构以完全无监督的方式学习不同图的两个嵌入空间之间的交叉图对齐。该框架学习每个图的嵌入空间，然后尝试通过对抗性训练对齐这两个空间，然后进行细化过程。我们进一步将我们的UAGA方法扩展到增量UAGA(iUAGA)，该方法基于伪锚定链接迭代地显示未观察到的用户链接。这可以用来进一步提高嵌入质量和对齐精度。</p><p>文章只使用两个图的结构信息，首先，利用无监督图嵌入方法分别学习源图和目标图的单个特征表示。接下来，它利用对抗性的训练来学习a 从源嵌入空间到目标嵌入空间的线性映射，然后进行细化过程，从得到的共享嵌入空间中得到伪节点对应 并使用来自[17]的封闭形式的解决程序解决方案对映射进行微调。最后，我们将UAGA方法扩展到增量UAGA(iUAGA)，以提高嵌入质量和对齐 性能迭代。</p><p><em>hub问题，即在高维嵌入空间中，一定数量的节点成为许多其他节点的枢纽和最近邻，如果不解决，将使 通过最近的搜索进行的对齐更加困难和不准确。</em></p><ul><li>定义一：(Graph Alignment Problem)给定两个社交网络<span class="math inline">\(G_s=(V_s,E_s)\)</span>以及<span class="math inline">\(G_t=(V_t,E_t)\)</span>，没有先验的anchor links，目的就是要找到隐藏的对齐的节点。</li></ul><p>在本文中，使用deepwalk来学习源网络和目标网络的graph embedding，定义为<span class="math inline">\(Z_s,Z_t∈R^{|V| \times d}\)</span>。</p><p><img src="../images/UAGA/UAGA模型图.png"></p><p>按照流程图，首先用无监督学习的方法学习图的特征表示，然后我们利用领域对抗性训练来学习W的初始代理。接下来，我们利用学习到的W来选择伪锚点链接来进行映射细化。最后，我们利用细化的映射W将所有源节点映射到目标潜在特征空间。我们通过改变空间的度量来提高比图对齐的性能，这将导致在数据密集的区域中分布更多的这些节点。</p><p>在本文中，我们提出学习一个映射，不需要任何形式的跨域监督之间，这样两个不同的嵌入空间可以很好地对齐。</p><p>令<span class="math inline">\(Z_s= \left\{ z_s^1,...,z_s^n\right\}\)</span>以及<span class="math inline">\(Z_t= \left\{ z_t^1,...,z_t^m\right\}\)</span>为源网络和目标网络的嵌入空间。如果我们知道跨网络的节点对齐（比方说<span class="math inline">\(z_s^i,z_t^i\)</span>是对齐的），我们能够学习到一个线性映射<span class="math inline">\(W∈R^{d \times d}\)</span>：</p><p><img src="../images/UAGA/W矩阵.png"></p><p>d是嵌入向量的维度，X和Y是两个对齐的矩阵，<span class="math inline">\(X,Y∈R^{d \times k}\)</span>，是由源网络和目标网络中找到的k个结点形成的。在测试阶段，任意源网络的embedding <span class="math inline">\(z_s^i\)</span>的转换可以被定义为：<span class="math inline">\(arg \max_{z_t^j∈Z_t}cos(Wz_s^i,z_t^j)\)</span>，注意，如果我们有地面-真节点对应，方程（2）可以直接用作监督学习的损失部分。</p><p>所提出的UAGA框架包括两个步骤：学习W的初始代理的领域对抗性训练，然后是利用最佳匹配节点来创建的细化过程 应用方程（2）的伪锚点链接。</p><h1 id="a.-domain-adversarial-training">A. domain-adversarial training</h1><p>在这一步中，我们的目标是要使得映射后的<span class="math inline">\(Z_s,Z_t\)</span>难以区分。（用的是GAN的方法）首先我们定义一个判别器，它旨在区分从<span class="math inline">\(WZ_s={W_{z^1_s}、W^{z^2_s}、..，W_{z^n_s}}\)</span>和<span class="math inline">\(Z_t\)</span>中随机采样的元素。映射W可以看作是生成器，通过使嵌入特征WZs和Zt变得无法区分，来防止鉴别器做出准确的预测。因此，域对抗训练过程是一个双人博弈，其中第一个玩家是训练区分源域和目标域，第二个玩家是同时被训练来混淆鉴别器的生成器。</p><p>给定映射W，鉴别器由<span class="math inline">\(\theta_D\)</span>通过最小化以下目标函数进行优化：</p><p><img src="../images/UAGA/鉴别器优化方程.png"></p><p>前一项表示的含义是：嵌入<span class="math inline">\(z_s^i\)</span>来自源嵌入空间，给定鉴别器，映射W的目的是通过最小化以下目标函数来欺骗鉴别器精确预测嵌入的原始域的能力：</p><p><img src="../images/UAGA/W的优化方程.png"></p><p>为了训练我们的模型，我们遵循标准的生成对抗网(GANs)[30]训练程序，它交替地和迭代地优化鉴别器θD和映射W，使其分别最小化LD和LW。</p><h1 id="b.-the-hubness-problem-and-refinement-procedure">B. the hubness problem and refinement procedure</h1><p>域对抗训练步骤学习一个匹配全局源和目标嵌入空间的映射函数W，而不考虑数据分布背后的复杂的多模结构。换句话说，在我们的无监督场景中，两组节点之间的细粒度的点到点约束（即没有两条边共享一个共同的端点）没有被明确地强制执行。</p><p>为了解决上述挑战，我们提出了一个细化过程，使细粒度的点对点与生成的伪锚定链路的图对齐。细化的优点 我们可以从两个方面得出结论。首先，通过引入额外的监督(伪锚链，我们在领域对抗训练步骤中规避了潜在的模式崩溃问题(这在传统gan中是一个臭名昭著的问题) )，以适用于点对点的对齐。第二，在选择伪锚链接时，我们在确定了相互关系中所谓的hub问题（即点倾向于高维空间中许多点的最近邻）[31] 通过引入一个交叉图相似度缩放(CGSS)方案。</p><p>我们使用在之前的领域对抗性训练步骤中学习到的W作为初始代理，随后构建许多伪造的anchor links，为了获得高质量的伪造的anchor links，我们只保留在<span class="math inline">\(Z_s和Z_t\)</span>中相互是最邻近的邻居的结点。然而，最近邻通常是不对称的：<span class="math inline">\(z_t\)</span>是<span class="math inline">\(z_s\)</span>的k最邻不代表<span class="math inline">\(z_s\)</span>是<span class="math inline">\(z_t\)</span>的k近邻。在嵌入特征空间中，这将导致一种不利于基于最近邻规则对齐节点嵌入的现象：嵌入空间中的一些节点，我们称之为枢纽，ar e更有可能是许多其他节点的最近邻居，但其他节点（称为反集线器）并不是任何节点的最近邻居。</p><p>我们使用CGSS来缓解所谓的hubness问题，我们考虑一个二部邻域图，其中给定锚链的每个节点都连接到另一个图中的K个最近邻。在二部图中，与映射的源节点嵌入关联<span class="math inline">\(Wz_s\)</span>的邻域被表示为<span class="math inline">\(N_T(Wz_s)\)</span>。<span class="math inline">\(N_T(Wz_s)\)</span>所有的K个元素是来自源网络的结点。源嵌入z与目标邻域的平均相似度表示为：（每个节点都连接到目标网络的k个最近邻，就是说在原网络中i和j是k近邻的话，那么在目标网络中他们也是k近邻的话那么就被保留为伪造的anchor links）</p><p><img src="../images/UAGA/邻域平均相似度.png"></p><p>cos表示cos相似度，同理可以定义目标embedding到其源网络邻居的平均相似度。我们使用有效的最近邻算法[32]来计算所有的源节点和目标节点的嵌入。形式上，我们利用这些相似性来定义一个跨域相似性度量CGSS（.，.）在映射的源嵌入和目标嵌入之间，</p><p><img src="../images/UAGA/跨域相似度度量.png"></p><p>CGSS方案的动机是直观的：它增加了与孤立节点嵌入相关的相似性，而减少了位于密集区域的嵌入的相似性。换句话说，我 T鼓励选择位于低密度区域的节点作为伪锚定链接。形式上，映射W通过方法进一步细化：我们应用方程（2） 或链接来细化W。（度越大的结点对于对齐的影响干扰越大）</p><h1 id="c.-orthography-constraints">C. orthography constraints</h1><p>在我们的工作中，我们额外提出了一个简单的更新步骤，使映射矩阵W在训练进行时保持接近一个正交矩阵。正交约束的优势如下：（1）分别保留源特征嵌入和目标特征嵌入的个体特征和质量；（2）随着训练的进行，稳定了学习过程；（3）保持点积以及它们的L2距离。具体来说，我们的模型将通过使用以下更新策略进行迭代和交替更新</p><p><img src="../images/UAGA/W的更新公式.png"></p><p>利用正交约束，方程（2）可以归结为普罗克鲁斯问题，就是一个矩阵的逼近问题，它的解就是<span class="math inline">\(YX^T\)</span>的奇异值分解值：</p><p><img src="../images/UAGA/W奇异值分解.png"></p><p>为了进一步有效地利用伪锚定链接来学习更准确的映射，我们进一步提出了一个增量的UAGA(iUAGA)程序，它逐步选择一组伪锚定链接，然后利用它们通过使用方程（2）来细化映射W。如果满足以下两个条件，我们选择一个伪锚定链路。首先，我们要求跨域相似度（即方程（5））应该超过阈值参数，我们设为 在实验中分别是0.7或0.75。第二个要求是根据CGSS，嵌入特征对是相互最近的邻居。我们假设，除非它们是相互的最近邻，否则成对的关系是不可靠的。这两个要求减少了伪锚定链路的数量，但提高了其精度和最终的对齐性能。伪造的铆钉节点表示为<span class="math inline">\(\left\{\widehat T=(v_s,v_t)|v_s∈V_s,v_t∈V_t\right\}\)</span>。</p><p>另一个问题就是一些存在的边可能因为没有显式的建立或者没有爬取到而并不能观察到，为了解决这个问题，在此基础上，我们的方法提出了通过伪标记许多可靠的锚定链路来逐步弥补交叉图监督的不足，这有助于显示了未被观察到的边缘（图内伪用户链接）。理由是，如果两个节点在一个图中不连接，但它们的对应节点（根据伪锚点链接）在另一个图中链接，那么在中间加一条边是可行的 在现在的图中，</p><p><img src="../images/UAGA/添加边的示例.png"></p><p>通过利用伪标记的用户链接，我们可以进一步提高图的嵌入质量和最终的对齐性能。它背后的原因是扩展的图可以提供 更丰富的结构信息，可用于图形的嵌入和映射。</p><p>形式上给定两个图<span class="math inline">\(G_s,G_t\)</span>，具有伪造的锚定节点<span class="math inline">\(\widehat T\)</span>，<span class="math inline">\(G_s\)</span>的扩展图<span class="math inline">\(\widehat G_s\)</span>可以被表示为：</p><p><img src="../images/UAGA/源网络的扩展图.png"></p><p>算法流程如下：</p><p><img src="../images/UAGA/算法流程.png"></p><p>整个算法的流程就是：首先得到两个图的各自的node embedding，然后进行GAN的参数更新，得到一个对齐矩阵W。然后获取一个伪造的anchor links集合（如果互为k临近的话那么就是符合要求的），随后细化映射W。随后按照公式7扩展源网络和目标网络，最后重置Gs,Gt和T。最后使用公式5来对齐网络。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="论文笔记" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>CONE-Align笔记</title>
    <link href="http://yoursite.com/2022/04/21/CONE-Align%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2022/04/21/CONE-Align%E7%AC%94%E8%AE%B0/</id>
    <published>2022-04-21T10:15:54.000Z</published>
    <updated>2022-05-30T02:07:35.822Z</updated>
    
    <content type="html"><![CDATA[<p><a id="more"></a></p><p>现有的无监督网络对齐方法可以找到分解节点邻域的次优对齐，即不保持匹配的邻域一致性。为了改进这一点，提出了CONE- Align，用节点嵌入建模网络内接近，并在对齐嵌入子空间后使用它们来匹配跨网络的节点。</p><p>许多的无监督网络对齐方法(FINAL,NETALIGN,REGAL)不能够实现匹配的邻域一致性：在一个图中接近的节点通常与在另一个图中接近的节点不匹配。REGAL使用结点嵌入捕获每个节点在网络中的结构身份，但是相邻的节点可能没有类似的结构角色，这导致了非常不一样的embeddings可能在另一个图中匹配得很远，违反了匹配的邻域一致性。CONE-Align： <strong>CON</strong>sistent <strong>E</strong>mbedding-based Network <strong>Align</strong>ment.我们使用著名的接近保持节点嵌入方法来学习每个图中邻近节点的相似节点嵌入。然而，由于节点在图之间并不接近，所以这些节点满足了 主节点是可转换的，不同图中的节点将被嵌入到不同的子空间中。因此，我们对图的嵌入子空间进行对齐，然后利用嵌入相似度来匹配节点。由于每个图中的相邻节点将有相似的嵌入，因此它们将被匹配 电解到另一个图的相似部分。因此，我们有最好的cone对齐：匹配邻域一致性和交叉图可比性。</p><p>大多数嵌入目标都是在单个图中建模接近性，如deepwalk，node2vec；结构嵌入方法捕获一个节点的结构角色，而不依赖于其与特定节点的接近程度，这种独立性使得嵌入在各个图之间具有可比性，例如struct2vec，xNetMF。</p><p>对于每一幅图，假设结点的个数为n（没有的话就补上），对于每个图<span class="math inline">\(G_i\)</span>，创建一个<span class="math inline">\(Y_i∈R^{n \times d}\)</span>表示结点的embedding matrix。网络对齐是找到一个方程<span class="math inline">\(\pi: V_1 \to V_2\)</span>或者一个矩阵<span class="math inline">\(P\)</span>其中<span class="math inline">\(p_{i,j}\)</span>表示的是在1网络中的结点i和2网络中的结点j的相似度。方程<span class="math inline">\(\pi\)</span>可以从P中找到，例如贪心策略：<span class="math inline">\(\pi(i) = arg \ max \ x_jp_{i,j}\)</span></p><p><span class="math inline">\(N_{G_1(i)}\)</span>表示在网络1中的节点i的邻居，定义i的"mapped neighborhood"在<span class="math inline">\(G_2\)</span>中为：经过<span class="math inline">\(\pi\)</span>映射的那个在<span class="math inline">\(G_2\)</span>中对应结点的邻居。<span class="math inline">\(\widetilde N_{G_2}^{\pi}(i)=\left\{j∈V_2:\exist k ∈ N_{G_1}(i) \ s.t. \pi(k)=j\right\}\)</span>，我们定义匹配邻域一致性MNC（Jaccard系数）</p><p><img src="../images/CONE/匹配邻域一致性.png"></p><p>问题定义：给定两个图：<span class="math inline">\(G_1和G_2\)</span>，没有先验知识，要恢复其对齐<span class="math inline">\(\pi\)</span>与此同时实现更高的MNC。</p><p><img src="../images/CONE/问题定义.png"></p><p>整体框架如下图所示：首先使用结点嵌入模拟图内的结点相似度，然后我们对齐嵌入空间以获得图之间的可比性。然后匹配两个网络中的结点用最相似的节点嵌入。</p><p><img src="../images/CONE/整体框架.png"></p><h2 id="step1node-embedding">1. step1：node embedding</h2><p>对于每个输入的图独自的获得其结点的嵌入<span class="math inline">\(Y_1,Y_2 ∈ R^{n \times d}\)</span>，我们只需要嵌入来保持图内节点的邻近性，每个图中的相邻节点都有相似的嵌入，在使用嵌入相似性时将被近距离映射。这有力地保留了MNC：即使节点由于缺少边[7]而不是邻居，许多节点嵌入算法也可以保持它们共享的任何高阶接近性。</p><h2 id="step2-embedding-space-alignment">2. step2： embedding space alignment</h2><p>由于嵌入目标的不变性，两个图的节点嵌入Y1和Y2可以相对于相互平移、旋转或重新缩放。在步骤2中对齐了两个嵌入子空间，我们解决了两个问题：</p><ol type="1"><li><p>procrustes：如果结点的对应关系已知，我们可以从正交矩阵<span class="math inline">\(O^d\)</span>中找到一个线性的变换Q。Q对齐结点嵌入矩阵的列，例如嵌入空间。可以被定义为解决下述的问题：</p><p><img src="../images/CONE/procrustes问题.png"></p><p>它的解为<span class="math inline">\(Q^*=UV^T\)</span>，其中<span class="math inline">\(U \Sigma V^T\)</span>是<span class="math inline">\(Y_1^TY_2\)</span>的SVD分解。</p></li><li><p>wasserstein：如果嵌入空间变化已知，可以从排列矩阵<span class="math inline">\(P^n\)</span>的集合中求解最优的节点对应关系P。P对齐节点嵌入矩阵的行，也就是结点。可以使用 Sinkhorn algorithm来最小化wasserstein distance：</p><p><img src="../images/CONE/wasserstein.png"></p></li><li><p>wasserstein procrustes：由于我们既不知道对应关系，也不知道转换，所以我们将这些问题结合起来：</p><p><img src="../images/CONE/解决方案.png"></p><p>我们等价的解决了<span class="math inline">\(max_{p∈p^n},max_{Q∈O^d}\)</span>的迹<span class="math inline">\(trace(Q^TY_1^TPY_2)\)</span>，通过一种随机优化方案，交替wasserstein以及procrustes问题。对于第T次迭代，我们使用目前的embedding转换矩阵Q对小批次的每次b个embeddings也就是<span class="math inline">\(Y_{1_t},Y_{2_t}\)</span>来找到一个匹配<span class="math inline">\(P_t\)</span>，使用sinkhorn算法λ正则化系数。我们随后使用wasserstein procrustes距离的梯度来更新Q。算法1如下：</p><p><img src="../images/CONE/alignment-embedding.png"></p></li><li><p>convex initialization：为了初始化上述非凸过程，我们转向了一个经典的凸图匹配公式：</p><p><img src="../images/CONE/凸初始化.png"></p><p>其中<span class="math inline">\(B^n\)</span>是<span class="math inline">\(p^n\)</span>的凸包，我们可以通过Frank-Wolfe algorithm找到全局最小的<span class="math inline">\(P^*\)</span>，经过<span class="math inline">\(n_0\)</span>次迭代，以及sinkhorn算法以<span class="math inline">\(λ_0\)</span>为正则化参数。使用<span class="math inline">\(Y_1和P^*Y_2\)</span>，以及一个初始的Q可以由正交的procrustes公式2生成。</p></li><li><p>complexity consideration：</p></li></ol><h2 id="step-3matching-nodes-with-embeddings">step 3：matching nodes with embeddings</h2><p>在将嵌入与最终的变换Q对齐后，在步骤3中，我们根据欧氏距离将𝐺1中的每个节点与𝐺2中的最近邻进行匹配。我们可以使用缩放修正来减轻“hub度”，即许多节点有相同的最近邻[11]，但我们发现没有必要。在[14]之后，我们使用一个𝑘-d树来进行快速最近邻搜索 h在<span class="math inline">\(Y_1Q\)</span>和<span class="math inline">\(Y_2\)</span>之间。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="论文笔记" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>G-ALIGN代码解析</title>
    <link href="http://yoursite.com/2022/03/31/G-ALIGN%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/"/>
    <id>http://yoursite.com/2022/03/31/G-ALIGN%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/</id>
    <published>2022-03-30T21:16:18.000Z</published>
    <updated>2022-05-30T02:07:45.708Z</updated>
    
    <content type="html"><![CDATA[<p><a id="more"></a></p><h1 id="embedding部分">1. embedding部分</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weight</span><span class="params">(modules, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Weight initialization</span></span><br><span class="line"><span class="string">    :param modules: Iterable of modules</span></span><br><span class="line"><span class="string">    :param activation: Activation function.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> modules:</span><br><span class="line">        <span class="keyword">if</span> isinstance(m, nn.Linear):<span class="comment">#判断m是否为nn.Linear类型，线性层</span></span><br><span class="line">            <span class="keyword">if</span> activation <span class="keyword">is</span> <span class="literal">None</span>:<span class="comment">#没有激活函数的话就初始化不加增益</span></span><br><span class="line">                m.weight.data = init.xavier_uniform_(m.weight.data)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                m.weight.data = init.xavier_uniform_(m.weight.data,gain=nn.init.calculate_gain(activation.lower()))</span><br><span class="line">            <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                m.bias.data = init.constant_(m.bias.data, <span class="number">0.0</span>)</span><br></pre></td></tr></table></figure><p>这里涉及到的是一个权重的初始化的步骤，在神经网络中的初始化步骤很重要。初始化应该既保证输入输出的差异性，又能让model稳定而快速的收敛。描述差异性的方法就是数学中的方差。对于每个神经元的输入z，<span class="math inline">\(z=\sum_{i=1}^nw_ix_i\)</span>，其中n是上一层中的神经元的个数。</p><p>那么两个随机变量的方差的公式为：<span class="math inline">\(Var(w_ix_i)=E[w_i]^2Var(x_i)+E[x_i]^2Var(w_i)+Var(w_i)Var(x_i)\)</span>，可以知道如果<span class="math inline">\(E[x_i]=E[w_i]=0\)</span>，能通过批量归一化满足这一条件，那么就有：</p><p><span class="math inline">\(Var(z)=\sum_{i=1}^nVar(x_i)Var(w_i)\)</span>，如果随机变量独立同分布的话，<span class="math inline">\(Var(z)=nVar(w)Var(x)\)</span>。一般来讲，类别空间相较于样本空间更加稠密，所以反向传播的误差到样本空间将显得微不足道。我们要让样本空间与类别空间的分布差异不要太大，也就是方差尽可能相等。</p><p>Xavier初始化方法就是：</p><p><img src="../images/G-ALIGN源码分析/Xavier初始化.png"></p><p>gain参数就是可选择的缩放的参数，总之以上就是初始化为了让神经网络更加健壮的手段。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="源码解析" scheme="http://yoursite.com/tags/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>Java并发</title>
    <link href="http://yoursite.com/2022/03/26/Java%E5%B9%B6%E5%8F%91/"/>
    <id>http://yoursite.com/2022/03/26/Java%E5%B9%B6%E5%8F%91/</id>
    <published>2022-03-25T19:47:36.000Z</published>
    <updated>2022-05-30T02:08:02.882Z</updated>
    
    <content type="html"><![CDATA[<h1 id="进程与线程">1. 进程与线程</h1><p><a id="more"></a></p><h2 id="进程">进程</h2><ul><li>程序是由指令和数据组成的，但是这些指令要运行，数据要读写就必须将指令加载至CPU，数据加载至内存。在指令运行的过程中还需要用到磁盘网络设备等。进程就是用来加载指令，管理内存，管理IO的</li><li>当一个程序被运行时，从磁盘加载这个程序的代码至内存，这时就开启了一个进程</li><li>进程可以视为程序的一个实例，大部分程序可以同时进行多个进程（例如记事本，画图等），也有的程序只能启动一个实例进程（如网易云音乐等）</li></ul><h2 id="线程">线程</h2><ul><li>一个进程之内可以分为一到多个线程</li><li>一个线程就是一个指令流，将指令流中的一条条指令以一定的顺序交给CPU执行</li><li>Java中，线程作为最小的调度单位，进程作为资源分配的最小的单位。在windows中进程时不活动的，只是作为线程的容器。</li></ul><h2 id="进程和线程的对比">进程和线程的对比</h2><ul><li>进程基本上是相互独立的，而线程存在于进程内，是进程的一个子集</li><li>线程拥有共享的资源，如内存空间等，供其内部的线程共享</li><li>进程间通信较为复杂，同一台计算机的进程的通信称为IPC，不同计算机之间的进程通信，需要通过网络，并遵守共同的协议，例如HTTP</li><li>线程通信相对简单，因为他们共享进程中的内存，一个例子是多个进程可以访问同一个共享变量</li><li>线程更加轻量级，线程上下文切换成本一般要比进程切换低</li></ul><h1 id="并行与并发">2. 并行与并发</h1><p>单核CPU下，线程实际上还是串行执行的。操作系统中有一个组件叫做任务调度器，将CPU的时间片（windows下最小为15毫秒）分给不同的线程使用，只是由于CPU在线程间（时间片很短）的切片非常快，人类感觉是同时运行的。微观串行，宏观并行。将线程轮流使用CPU的做法称为并发。</p><p>在多核CPU下，每个核都能调度线程，这时候线程可以是并行的。</p><ul><li>并发是同一时间应对多件事情的能力</li><li>并行是同一时间动手做多件事情的能力</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;进程与线程&quot;&gt;1. 进程与线程&lt;/h1&gt;
&lt;p&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Java并发" scheme="http://yoursite.com/tags/Java%E5%B9%B6%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>Java知识补充</title>
    <link href="http://yoursite.com/2022/03/24/Java%E7%9F%A5%E8%AF%86%E8%A1%A5%E5%85%85/"/>
    <id>http://yoursite.com/2022/03/24/Java%E7%9F%A5%E8%AF%86%E8%A1%A5%E5%85%85/</id>
    <published>2022-03-24T11:50:24.000Z</published>
    <updated>2022-05-30T04:28:25.259Z</updated>
    
    <content type="html"><![CDATA[<h1 id="junit单元测试">1. Junit单元测试</h1><p><a id="more"></a></p><p>测试的分类：黑盒测试（无需代码，给定输入看输出），白盒测试（需要写代码看代码的执行流程）</p><h1 id="java反射">2. Java反射</h1><p><img src="../images/Java基础知识/Java程序运行的步骤.png"></p><p>Java程序的运行机制如上图所示，</p><p>反射是框架设计的灵魂，将类的各个组成部分封装为其他对象，这就是反射机制。</p><p>好处又：（1）可以在程序运行过程中，操作这些对象。（2）可以解耦，提高程序的可扩展性。</p><p>获取Class对象的方式：</p><ol type="1"><li><p>Class.forName("全类名")：将字节码文件加载进内存，返回class对象</p><p>多用于配置文件，将类名定义在配置文件中，读取文件，加载类</p></li><li><p>类名.class：通过类名的属性class获取</p><p>多用于参数的传递</p></li><li><p>对象.getClass()：getClass()方法在Object类中定义。</p><p>多用于对象的获取字节码方式</p></li></ol><p>同一个字节码文件(*.class)在一次程序的运行过程中，只会被加载一次，不论通过哪一种方式获取的Class对象都是同一个。</p><h1 id="java继承">3. java继承</h1><p>多个类中存在相同的属性和行为的时候，将这些内容抽取到单独一个类中，那么多个类无需再定义这些属性和行为，只需要继承那个类即可。多个类可以称为子类，单独这个类称为父类。子类可以直接访问父类中的非私有的属性和行为。</p><p>继承的作用：（1）提高代码的复用性；（2）让类与类之间产生了关系，是多态的前提。</p><p>java只支持单继承，不支持多继承。java支持多重继承。使用继承的时候不要为了继承部分功能，而去使用继承。</p><p>super是一个关键字，代表父类的存储空间标识，可以理解为父类的引用。super和this的用法类似，this表示当前对象的引用，谁调用就代表谁。super代表当前子类对父类的引用。</p><p>当子类和父类出现同名成员的时候，可以使用super进行区分；子类要调用父类的构造函数时，可以使用super语句。</p><p>方法的重写，子类中出现与父类一模一样的方法时，除了权限修饰符，权限符大于private，返回值类型，方法名和参数列表一样。这个时候会出现覆盖的操作，也称为重写。</p><p>覆盖的时候必须要注意：</p><ul><li>覆盖时，子类方法的权限一定要大于等于父类方法权限</li><li>静态只能覆盖静态</li></ul><p>覆盖的使用场景：当子类需要父类的功能，而功能主体子类有自己的特有内容的时候，可以复写父类中的方法。</p><p>方法的重写和重载的区别：方法的重写在子类方法与父类方法一样，除了权限修饰符；而重载用在同一个类中各个方法方法名相同，参数列表不同的情况。</p><p>子父类中构造的用法：</p><ul><li>子类的过程初始化的过程中，首先回去执行父类的初始化动作。因为子类的构造方法中默认有一个super()，子类要使用父类的成员变量，这个初始化，必须在子类初始化之前完成。所以，子类的初始化过程中，会先执行父类的初始化。</li><li>如果父类没有无参构造方法：使用super调用父类的待参构造（推荐方式）</li></ul><p>执行顺序：父类静态代码块→子类静态代码块→父类构造代码块→父类构造方法→子类构造代码块→子类构造方法</p><h2 id="final关键字">final关键字</h2><p>final是一个关键字，用来修饰类，成员变量和成员方法。</p><p>特点：</p><ul><li>它修饰的类不能被继承</li><li>他修饰的成员变量是一个常量</li><li>它修饰的成员方法是不能被子类重写的</li></ul><p>final修饰的常量定义一般都有书写的规范，被final修饰的常量名称，所有字母都要大写</p><p>final修饰成员变量，必须初始化，初始化有两种：</p><ul><li>显式初始化</li><li>构造方法初始化</li></ul><p>final和private的区别：</p><ul><li>final修饰的类可以访问，private不能修饰外部类，但是可以修饰内部类</li><li>final修饰的方法不能被子类重写，private修饰的方法表面上看可以被子类重写，其实是不可以的，子类是看不到父类的私有方法的。</li><li>final修饰的变量只能在显示初始化或者构造函数初始化时赋值一次，以后不能修改；private修饰的变量，也不允许直接被子类或包中的其他类访问或者修改，但是可以通过get和set方法对其改值和取值。</li></ul><h2 id="多态">多态</h2><p>对象在不同的时刻表现出来的不同状态。</p><p>多态的前提：</p><ul><li>要有继承或者实现关系</li><li>要有方法的重写</li><li>要有父类引用指向子类对象</li></ul><p>程序中体现为：父类或者接口的引用指向或者接收自己的子类对象</p><p>好处：多态的存在提高了程序的可扩展性和后期可维护性</p><p>弊端：父类的调用时只能调用父类里面的方法，不能调用子类的特有方法，因为并不清楚将来将有什么样的子类来继承。</p><p>多态成员的特点：</p><ul><li><p>成员变量：</p><p>编译时期：看引用变量所属的类中是否有所调用的变量</p><p>运行时期：也是看引用型变量所属的类是否有调用的变量</p><p>成员变量无论编译还是运行都看引用型变量所属的类，简单记为成员变量编译和运行都看左边</p></li><li><p>成员方法：</p><p>编译时期：要看引用变量所属的类中是否有所调用的成员</p><p>运行时期：要看对象所属的类中是否有所调用的成员。如果父子出现同名的方法，因为方法有覆盖的特性，<strong>编译看左边，运行看右边</strong></p></li><li><p>静态方法：</p><p>编译时期：看的引用型变量所属的类中是否有所调用的变量</p><p>运行时期：也是看引用型变量所属的类是否有调用的变量</p><p><strong>编译和运行都看等号左边</strong></p></li></ul><p><strong>一定不能够将父类的对象转换成子类类型</strong></p><p>父类的引用指向子类对象，该引用可以被提升，也可以被强制转换，多态自始至终都是子类对象在变化。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">polymorphism</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        win1 a = <span class="keyword">new</span> win1(<span class="string">"win1"</span>);</span><br><span class="line">        a.tostring();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">win</span></span>&#123;</span><br><span class="line">    String name;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">win</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        name = <span class="string">"win"</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">win</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">tostring</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"This is win"</span>);</span><br><span class="line">        Tostring();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Tostring</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"This is win"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">win1</span> <span class="keyword">extends</span> <span class="title">win</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">win1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    win1(String name) &#123;</span><br><span class="line">          <span class="keyword">this</span>.name = name;</span><br><span class="line"><span class="comment">//        super(name);</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">tostring</span><span class="params">(String name)</span></span>&#123;<span class="comment">//重载父类中的tostring</span></span><br><span class="line">        System.out.println(<span class="string">"this is win1"</span>);</span><br><span class="line">        Tostring();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Tostring</span><span class="params">()</span></span>&#123;<span class="comment">//重写Tostring方法</span></span><br><span class="line">        System.out.println(<span class="string">"this is win1"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实现多态的很重要的一点就是向上转型，在多态中需要将子类的引用赋给父类对象，只有这样该引用才能够具备既能调用父类的方法，又能调用子类的方法。什么是向上转型？假设：A b = new A();实例化了一个A的对象，但是如果定义A b= new a();(a是A的子类)定义了一个A的类型b，但是是由a对象来实例化的，也就是用子类来实例化的。有时候子类中有需要对父类中的某些方法进行重写，然后调用方法的时候就会调用子类重写的方法而不是原本父类的方法。向上转型后，子类单独定义的方法会丢失（即子类重载父类中的方法），而子类中重写了父类的方法，当调用的时候，就会调用重写的方法。</p><p>上述的代码win1重载了tostring()方法，重写了Tostring()方法，因为向上转型的原则，先会调用父类中的tostring()方法（子类中的tostring()重载后丢失），在调用子类中的Tostring()方法。</p><p>如果同时将子类中的两个方法都重载：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">tostring</span><span class="params">(String name)</span></span>&#123;<span class="comment">//重载父类中的tostring</span></span><br><span class="line">    System.out.println(<span class="string">"this is win1"</span>);</span><br><span class="line">    Tostring();</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Tostring</span><span class="params">(String name)</span></span>&#123;<span class="comment">//重载Tostring方法</span></span><br><span class="line">    System.out.println(<span class="string">"this is win1"</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 输出:</span></span><br><span class="line"><span class="comment">* this is win</span></span><br><span class="line"><span class="comment">* this is win</span></span><br><span class="line"><span class="comment">**/</span></span><br></pre></td></tr></table></figure><p>那么输出的都是父类的方法</p><h2 id="抽象">抽象</h2><p>抽象就是从多个事物中将共性的，本质的内容抽象出来。</p><p>抽象类：java中可以定义没有方法体的方法，该方法的具体实现由<strong>子类</strong>完成，该方法称为抽象方法，包含抽象方法的类就是抽象类。</p><p>由来：多个对象都具备相同的功能，但是功能具体内容有所不同，那么在抽取的过程中，只抽取了功能定义，并未抽取功能主体，那么只功能声明，没有功能主体的方法称为抽象方法。</p><p>抽象类的特点：</p><ul><li>抽象方法一定在抽象类中</li><li>抽象方法和抽象类都必须被abstract关键字修饰</li><li>抽象类不可以用new创建对象，因为调用抽象方法没有意义</li><li>抽象类中的抽象方法要被使用的话，必须由子类复写其所有的抽象方法后，建立子类对象调用；如果子类只覆盖了部分的抽象方法，那么该子类还是一个抽象类</li><li>抽象类中可以有抽象方法，也可以有非抽象的方法，抽象方法用于子类的实例化</li><li>如果一个类是抽象类，那么继承它的子类，要么是抽象类，要么重写所有的抽象方法</li><li>特殊的，抽象类可以不定义抽象方法，这样做仅仅是不让该类建立对象</li></ul><p>抽象类的成员特点：</p><ul><li>成员变量：可以是变量，也可以是常量</li><li>构造方法：有构造方法</li><li>成员方法：可以是抽象方法，也可以是非抽象方法</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AbstractClass</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        cls2 cls2 = <span class="keyword">new</span> cls2();</span><br><span class="line">        cls2.test01();</span><br><span class="line"></span><br><span class="line">        cls3 cls3 = <span class="keyword">new</span> cls3();</span><br><span class="line">        cls3.test01();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">cls1</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">test01</span><span class="params">()</span></span>;<span class="comment">//抽取功能定义没有抽取功能主体</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">cls2</span> <span class="keyword">extends</span> <span class="title">cls1</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test01</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"剪刀"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">cls3</span> <span class="keyword">extends</span> <span class="title">cls1</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test01</span><span class="params">()</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"指甲刀"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>抽象类需要注意的有：</p><p>抽象类不能被实例化，但为什么还有构造函数？因为只要是class定义的类里面就有构造函数，抽象类中的函数是给子类实例化的。一个类没有抽象方法的话，如果不想被继承还不想被实例化，那么考虑定义为抽象类。</p><p>抽象类不能跟下面的关键词共存：</p><ul><li>final：final不可以被覆盖，但是如果方法被抽象那就需要被覆盖，所以冲突</li><li>private：如果函数私有了，子类无法直接访问，所以无法覆盖</li><li>static：不需要对象，类名就可以调用抽象方法，调用抽象方法没有意义</li></ul><h2 id="接口">接口</h2><p>接口是抽象方法和常量值的集合，从本质上讲，接口是一种特殊的抽象类，这种抽象类只包含常量和方法的定义，而没有变量和方法的实现。</p><p>格式：interface 接口名{}</p><p>接口的出现将多继承通过另一种形式表现出来，即多实现</p><p>实现：class 类名 implements 接口名{}</p><p>特点：</p><ul><li>接口不能被实例化</li><li>一个类如果实现了接口，要么是抽象类，要么实现接口中的所有方法</li></ul><p>接口成员的特点：</p><p>接口中的成员修饰符是固定的。</p><ul><li>成员变量：public static final，接口里定义的是全局变量，而且修饰符只能是这三个关键字，都可以省略，常量名要大写</li><li>成员方法：public abstract，接口里定义的方法都是抽象的，两个修饰符关键字可以省略。</li></ul><p>继承与实现的区别：</p><ul><li>类与类之间称为继承关系：因为该类无论是抽象的还是非抽象的，它的内部都可以定义非抽象方法，这个方法可以直接子类使用，子类继承即可。只能单继承，可以多层继承。</li><li>类与接口之间是实现关系：因为接口中的方法都是抽象的，必须由子类实现才可以实例化。可以单实现，还可以在继承一个类的同时实现多个接口<strong>(class) extends (class) implements (interface1,interface2…)</strong>）</li><li>接口与接口之间是单继承关系：一个接口可以继承另一个接口，并添加新的属性和抽象方法，并且接口可以多继承。</li></ul><p><strong>抽象类和接口的区别：</strong></p><p>成员变量：</p><ul><li>抽象类能有变量也可以有常量</li><li>接口只能有常量</li></ul><p>成员方法：</p><ul><li>抽象类可以有非抽象的方法，也可以有抽象的方法</li><li>接口只能有抽象的方法</li></ul><p>构造方法：</p><ul><li>抽象类有构造方法</li><li>接口没有构造方法</li></ul><p>类与抽象类和接口的关系：</p><ul><li>类与抽象类的关系是继承extends</li><li>类与接口的关系是实现implements</li></ul><p>接口的思想特点：</p><ol type="1"><li>接口是<em>对外暴露</em>的规则；</li><li>接口是程序的<em>功能扩展</em>；</li><li>接口的出现<em>降低耦合性</em>；(实现了模块化开发,定义好规则,每个人实现自己的模块,大大提高了开发效率)</li><li>接口可以用来<em>多实现</em>；</li><li><em>多个</em>无关的类可以实现同一个接口；</li><li>一个类可以实现<em>多个</em>相互直接没有关系的接口；</li><li>与继承关系类似，接口与实现类之间存在<em>多态性</em>。</li></ol><h2 id="内部类">内部类</h2><p>将一个类定义在另一个类中，里面那个类就成为内部类。</p><p><strong>访问特点</strong>：</p><ul><li>内部类可以直接访问外部类的成员，包括私有成员。</li><li>外部类要访问内部类的成员，必须要建立内部类的对象。 <strong>内部类分类及共性</strong>：</li></ul><p><em>共性</em>：</p><ul><li>内部类仍然是一个独立的类，在编译之后会内部类会被编译成独立的.class文件，但是前面冠以外部类的类名和$符号。</li><li>内部类不能用普通的方式访问。内部类是外部类的一个成员，因此内部类可以自由地访问外部类的成员变量，无论是否是private的。</li></ul><p><em>成员内部类</em></p><p>在外部类中有成员变量和成员方法，成员内部类就是把整个一个类作为了外部类的成员； 成员内部类是定义在<em><strong>类中方法外*</strong>的类； 创建对象的格式为：</em><strong>外部类名.内部类名 对象名 = 外部类对象.内部类对象*</strong>； 成员内部类之所以可以直接访问外部类的成员，那是因为内部类中都持有一个外部类对象的引用：*<strong>外部类名.this*</strong>； 成员内部类可以用的修饰符有final，abstract，public，private，protected，static.</p><p><em>静态内部类</em></p><p>静态内部类就是成员内部类加上静态修饰符static，定义在*<strong>类中方法外*</strong>。</p><p>在外部类中访问静态内部类有两种场景：</p><ul><li>在外部类中访问静态内部类中非静态成员：<em><strong>外部类名.内部类名 对象名 = 外部类名.内部对</strong>象</em>，需要通过创建对象访问；</li><li>在外部类中访问静态内部类中的静态成员：同样可以使用上面的格式进行访问，也可以直接使用*<strong>外部类名.内部类名.成员*</strong>。</li></ul><p>局部内部类</p><p>局部内部类是定义在方法中的类。</p><ul><li>方法内部类只能在定义该内部类的*<strong>方法内*</strong>实例化，不可以在此方法外对其实例化。</li><li>方法内部类对象不能使用该内部类所在方法的非final局部变量。</li></ul><p>可以用于方法内部类的修饰符有<em>final，abstract</em>；</p><p>静态方法中的方法内部类只能访问*<strong>外部的静态成员*</strong>。</p><p><em>匿名内部类</em></p><p>匿名内部类是内部类的简化写法，是建立一个带内容的外部类或者接口的子类匿名对象。 前提： 内部类可以继承或实现一个外部类或者接口。 格式： new 外部类名或者接口名(){重写方法}; 通常在方法的形式参数是接口或者抽象类，并且该接口中的方法不超过三个时，可以将匿名内部类作为参数传递。</p><h1 id="static关键字">4. static关键字</h1><p><!-- more --></p><p>通常来说，当创建类的时候，就是在描述那个类的对象的外观和行为，除非用new来创建那个类的对象，否则，实际上并未获得任何对象。执行new来创建对象的时候，类型存储空间才被分配，其方法才供外界调用。</p><p>上述方法无法解决两种情况：1. 只想为某特定域分配单一存储空间，而不去考虑究竟要创建多少对象，甚至根本就不创建任何对象； 2. 希望某个方法不与包含它的类的任何对象关联在一起，也就是说，即使没有创建对象，也能够调用这个方法。</p><p>解决这两个问题的方法就是使用static关键字，当声明一个对象是static时，就意味着这个域或方法不会与包含它的那个类的任何对象实例关联在一起。即使从未创建某个类的任何对象，也可以调用其static方法或访问其static域。</p><p>例如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StaticTest</span></span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span> i = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建两个类对象</span></span><br><span class="line">StaticTest st1 = <span class="keyword">new</span> StaticTest();</span><br><span class="line">StaticTest st2 = <span class="keyword">new</span> StaticTest();</span><br></pre></td></tr></table></figure><p>那么在java里面st1.i和st2.i指向同一个存储空间，这两个对象共享一个i。引用static变量既可以通过新建一个对象去引用，，也可以通过类名直接去引用</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StaticTest.i ++;</span><br></pre></td></tr></table></figure><p>使用类名来引用static变量是首选，这不仅是因为它强调了变量的static结构，而且在某些情况下它还为编译器进行优化提供了更好的机会。同理来说，static方法也可以这样来调用。</p><h1 id="java泛型">5. java泛型</h1><p>java泛型的目的之一就是用来指定容器需要持有什么类型的对象，而且由编译器来保证其类型的正确性。</p><h2 id="元组类库">元组类库</h2><p>由于return语句只能返回单个对象，因此需要考虑创建一个对象，用它来持有想要返回的多个对象。那么这个概念就叫做元组，它是将一组对象直接打包存储于其中的一个单一的对象。这个容器允许读取其中的元素，但是不允许向其中存放新的对象。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TwoTuple</span> &lt;<span class="title">A</span>, <span class="title">B</span>&gt;</span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> A first;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> B second;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TwoTuple</span><span class="params">(A first, B second)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.first = first;</span><br><span class="line">        <span class="keyword">this</span>.second = second;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"TwoTuple&#123;"</span> +</span><br><span class="line">                <span class="string">"first="</span> + first +</span><br><span class="line">                <span class="string">", second="</span> + second +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的代码中，虽然使用public的对象first和second，但是由于使用了final关键字，所以即使外界代码可以调用他，但是仍然不能更改他。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;junit单元测试&quot;&gt;1. Junit单元测试&lt;/h1&gt;
&lt;p&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Java" scheme="http://yoursite.com/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>JVM笔记</title>
    <link href="http://yoursite.com/2022/03/24/JVM%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2022/03/24/JVM%E7%AC%94%E8%AE%B0/</id>
    <published>2022-03-23T21:42:24.000Z</published>
    <updated>2022-05-30T02:08:20.179Z</updated>
    
    <content type="html"><![CDATA[<h1 id="内存结构介绍">1. 内存结构介绍</h1><p><a id="more"></a></p><p><img src="../images/JVM/内存结构简图.png"></p><h2 id="类加载器">类加载器</h2><p><img src="../images/JVM/类加载器.png"></p><ul><li>类加载器子系统负责从文件系统或者网络中加载class文件，class文件在文件开头有特定的文件标识</li><li>ClassLoader只负责class文件的加载，至于它是否可以运行，则有Execution Engine决定</li><li>加载的信息存放于一块称为方法区的内存空间，除了类的信息外，方法区中还会存放运行时常量池信息，可能还还包括字符串字面量和数字常量（这部分常量信息是Class文件中常量池部分的内存映射）</li></ul><p>类加载器ClassLoader的角色：</p><p><img src="../images/JVM/classLoader.png"></p><ul><li>1.class file存在于本地硬盘上，可以理解为设计师画在纸上的模板，而最终这个模板在执行的时候是要加载到JVM当中来根据这个文件实例化出n个一模一样的实例。</li><li>2.class file加载到JVM中，被称为DNA元数据模板，放在方法区</li><li>在.class文件-&gt;JVM -&gt;最终称为元数据模板，此过程就要一个运输工具，（类装载器ClassLoader），扮演一个快递员的角色</li></ul><p><img src="../images/JVM/类加载过程.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;内存结构介绍&quot;&gt;1. 内存结构介绍&lt;/h1&gt;
&lt;p&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="JVM学习" scheme="http://yoursite.com/tags/JVM%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Network Alignment with Holistic Embeddings笔记</title>
    <link href="http://yoursite.com/2022/03/24/Network-Alignment-with-Holistic-Embeddings%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2022/03/24/Network-Alignment-with-Holistic-Embeddings%E7%AC%94%E8%AE%B0/</id>
    <published>2022-03-23T20:17:30.000Z</published>
    <updated>2022-05-30T02:08:11.951Z</updated>
    
    <content type="html"><![CDATA[<p><a id="more"></a></p><p>一种新的端到端对齐框架，可以利用不同的模式来以一种有效的方式比较和对齐网络节点。为了利用网络上下文的丰富性，我们的模型为每个节点构建多个嵌入，每个嵌入捕获一种模态或类型的网络信息。然后我们设计了一个 后期融合机制，基于底层信息的重要性来结合学习到的嵌入。我们的融合机制允许我们的模型适应于各种类型的结构 输入网络。实验结果表明，我们的技术在真实数据和合成数据集上的精度方面优于最先进的方法，同时对各种噪声因子具有鲁棒性 。</p><p>矩阵分解的方法不能适用于大规模的网络，而基于embedding的方式具有以下的局限性：</p><ul><li>首先，将具有相似拓扑特征的网络节点分配给相似的嵌入；这意味着一个给定的网络节点很可能有一个类似于邻居的嵌入 这使得它很难区分这些节点。</li><li>大多数网络对齐器使用某些一致性的概念作为其模型的主干。例如，结构一致性要求节点对的邻域是相似的 两个网络。然而，这种严格的约束很难执行，特别是在现实世界的网络中。</li><li>现有的网络对齐模型只使用单一的嵌入；这限制了模型的容量，因为一个嵌入只能捕获网络的某些方面。</li></ul><p>节点属性仅提供一个本地视图，并且不能提供整个网络的全局视图。这损害了对齐的准确性，因为在焦油中存在几个具有相似局部信息的节点 获取图形。</p><p>为了解决上述挑战，我们提出了一个基于多嵌入的网络对齐模型。</p><p>构建三层嵌入：浅层嵌入，深层嵌入以及基于社区的嵌入(为网络结构的全局视图)</p><p><img src="../images/NAME/对齐示例.png"></p><p>作者提出了两种新的网络表示学习方法（专门用于网络对齐任务，包含属性信息，高阶相似性）：第一种方法是基于GCN的嵌入，它是对我们之前的工作[30]的改进，在那里我们重新设计了损失模块，以适应监督设置，从而促进了与其他嵌入的统一。第二种方法是全局社区感知嵌入，通过在节点嵌入中编码社区信息的，超越了现有的图形社区检测的工作。该方法还旨在克服局部最优问题，即所有节点都被分配到一个分区。类似于我们的工作是，但是我们的社区更好地保存拓扑，并利用节点本身的社区成员，而不是邻居锚。</p><p><img src="../images/NAME/NAME架构.png"></p><ul><li><p>问题1：属性网络对齐</p><p>给定两个属性网络<span class="math inline">\(G_s= (V_s,A_s,F_s) \ and \ G_s=(V_t,A_t,F_t)\)</span>，用来找到对齐矩阵S，表明<span class="math inline">\(G_s，G_t\)</span>之间的相似度。</p></li></ul><p>要满足的约束：属性一致性和拓扑一致性。</p><p>由于信息类型的许多基于嵌入的网络对齐器无法满足这两个一致性约束，因为信息类型的模态和信息丰富度的限制 信息到嵌入中。虽然有一些技术试图覆盖这两种类型的约束，但它们在单独的步骤中考虑它们，或者忽略了有价值的信息。</p><p>模型需要考虑属性还有结构噪声，直接考虑这些噪声可能会与一致性约束相违背。真对应节点和相似节点在拓扑结构和属性上的混淆是降低网络对齐器精度的主要因素之一。在选择真正对应的节点时，缺乏被纳入单一嵌入中的信息会导致短视的决策。</p><p>我们提出了两种新的嵌入方法来详尽地利用网络信息：一种基于GCN的嵌入方法，它统一了网络节点的多阶拓扑信息和属性信息，以及全局视图嵌入，捕获整个图结构社区中网络节点的成员。</p><p>三种embedding</p><ul><li>local structure：学习节点的一阶接近度信息</li><li>embedding based on a graph neural network：GCN模型来学习高阶属性和拓扑的信息</li><li>global community-aware embedding：全局社区的embedding</li></ul><h2 id="embedding-for-local-structure">embedding for local structure</h2><p>网络表示学习就是计算两个邻居节点共现的概率，<span class="math inline">\(z_i,z_j\)</span>是对应的embedding。</p><p><img src="../images/NAME/网络表示学习.png"></p><p>经过负采样：</p><p><img src="../images/NAME/网络表示学习的负采样.png"></p><p>表示空间协调，由于源网络和目标网络是独立嵌入的，因此需要一个映射函数Θ来协调这两个嵌入空间。映射函数Θ可以是线性函数，也可以是多层感知器，可以表示为以下的优化函数：</p><p><img src="../images/NAME/表示空间协调.png"></p><p><span class="math inline">\(z_v^s,z_v^t\)</span>表示在预先对齐（先验知识）集合T中的锚定节点<span class="math inline">\((v_s,v_t)\)</span>的embedding，损失函数的直观目的是保证在映射后，T中锚定节点的嵌入是相似的。调整后，可以根据源节点和目标节点的嵌入直接计算出目标节点之间的相关性。</p><h2 id="embedding-based-on-a-graph-neural-network">embedding based on a graph neural network</h2><p>为了加强局部结构嵌入，我们设计了一个特定的GCN模型，它同时编码结构和属性信息。两个锚节点的相似性取决于它们的拓扑属性和语义属性。</p><p>GCN的传递公式：</p><p><img src="../images/NAME/GCN第l层.png"></p><p>不使用ReLU而使用tanh，因为ReLU在信号为负值时丢失信息，因此不适合网络对齐任务。而且该论文使用的是GCN的多层的融合嵌入，作者提出了一个分层损失函数，保证两个标准：consistency-aware 以及anchor-aware。</p><ul><li><p>consistency-aware loss：该损失组件的目的是通过最小化相邻节点的层嵌入之间的距离，同时最大化不相关节点的层嵌入之间的距离，来整合一致性约束：</p><p><img src="../images/NAME/consistency-aware%20loss.png"></p></li><li><p>anchor-aware loss：这个损失组件的核心思想是确保源节点嵌入和目标节点嵌入属于一个共享的表示空间。这种损失迫使锚对在每个铺设层的嵌入 类似的：</p><p><img src="../images/NAME/anchor-aware%20loss.png"></p></li></ul><p>其中T是已知的anchor集合。</p><p>最终对于每一层的损失函数如下：</p><p><img src="../images/NAME/GCN每层的损失函数.png"></p><p>各层损失函数求和为：<img src="../images/NAME/各层损失函数求和.png"></p><p>源网络和目标网络的GCN模型对每一层使用相同的权重矩阵。该机制保证了源节点和目标节点的嵌入具有共同的嵌入空间。这就消除了对两个嵌入空间的协调步骤的要求。此外，该机制确保了满足一致性约束，因为相应的节点与 以共享的权值传播的相同的结构信息和属性信息将具有相同的嵌入。</p><h2 id="global-community-aware-embedding">global community-aware embedding</h2><p>在本节中，我们设计了另一个嵌入模型，该模型提供了基于社区结构的网络节点的全局视图，这为社区提供了另一个有价值的信息来源 对齐过程。</p><p>给定一个图<span class="math inline">\(G=(V,A)\)</span>，社区发现的目的就是找到一个集合划分<span class="math inline">\(C=\left\{C_1,...,C_m\right\}\)</span>就是m个不相交的社区。用一个membership matrix来表示划分,<span class="math inline">\(M∈\left\{0,1\right\}^{n \times m}\)</span>，具有n行表示节点，m列表示有m个社区。请注意，由于这些社区是不相交的，因此每个网络节点只属于一个社区，因此隶属度矩阵M中的每一行都是一个热门向量。检测社区的问题可以表示为寻找一个优化的成员矩阵的问题。</p><p>矩阵M不仅清楚地表示了网络节点的社区划分，还可以用来生成一个新的粗化邻接矩阵，其中“节点”为社区：</p><p><img src="../images/NAME/新的邻接矩阵.png"></p><p>在<span class="math inline">\(A^{com}\)</span>中，在主对角线上的元素<span class="math inline">\(A_{ii}^{com}\)</span>表示社区Ci中各节点之间的内部连接（边）的数量，非对角线元素<span class="math inline">\(A_{i,j}^{com}\)</span>表示社区Ci和Cj之间的交叉连接的数量。</p><p>为了学习优化后的成员矩阵M，我们提出使用一个Membership probability probability matrix <span class="math inline">\(P∈R^{n \times m}\)</span>，其中<span class="math inline">\(p_{i,j}\)</span>表示节点i属于社区j的可能性。这个概率矩阵可以被认为是隶属度矩阵的一个松弛版本（从二进制矩阵到实矩阵）。这种的好处有，（1）M可以从P中推断出来，通过给每个节点分配概率最高的社区.（2）实值矩阵的使用有助于促进学习过程。（3）概率向量可以作为网络节点的表示，类似于浅层嵌入技术中的嵌入矩阵.</p><p>由于矩阵M是离散的，而其松弛版本P是连续的，因此我们应用Gumbel-Softmax技术从P中采样M。</p><p><strong>Gumbel-Softmax：这是一种参数化的采样方法，在离散变量的采样中具有：将某一随机离散变量X变得对每一个维度概率可导的作用。一个离散变量X如果满足p(X=1)=0.2，P(X=2)=0.2，P(X=3)=0.5，如果想要得到一些服从这个分布的离散的x的值，一般的思路是按照这个概率去采样，采样一些x来用。这样的问题就是，采样出来的x只有值没有式子，在神经网络中，没有办法将x对P求导，也就没办法进行反向传播。能不能给一个以p1,p2,p3为参数的式子，让这个公式返回的结果是x的采样？找到的式子如下：</strong></p><p><img src="../images/NAME/Gumbel-Softmax.png"></p><p><strong>gi表示的是Gumbel噪声，这个噪声是用来使得z的返回结果是不固定的，最终得到的z向量是一个one hot向量，用这个向量乘以x的值域向量，得到的就是采样的x，上面的函数只有argmax函数不可导，用softmax函数代替</strong></p><p><img src="../images/NAME/z向量.png"></p><p><strong>这个式子里的参数 T 越小，z越接近one_hot向量。然后我们得到了一些可以对p求导的x的取样值，当然因为我们最后用的是softmax，所以x的值跟纯粹的取样也不完全一样，但比起直接求期望，我们至少得到了样本。这个过程相当于我们</strong>把不可导的取样过程，从x本身转嫁到了求取x的公式中的一项g上面，而g不依赖于p1,p2,p3<strong>。这样一来，x对p1,p2,p3仍然是可导的，而我们得到的x仍然是离散值的采样。目标达成。这样的采样过程转嫁的技巧有一个专门的名字，叫</strong>再参化技巧(reparameterization trick)。****</p><p>因为这是一个可微算子(而argmax算子是不可微的)，因此适用于我们的端到端神经网络模型。然后将M的每一行从P中计算出来如下：</p><p><img src="../images/NAME/从M采样P.png"></p><p>其中t是常数，g是Gumbel分布的样本，β是噪声超参数。当温度t接近于零时，一个来自Gumbel-Softmax分布的样本变成了一个单热向量，这有助于将概率矩阵P转移到M。</p><p><img src="../images/NAME/社区检测损失函数.png"></p><p>请注意，当集成到模型中时，损失函数接收的是连续的值，而不是整数的值，因为隶属度矩阵M是基于概率矩阵的估计版本 P.</p><p>网络对齐的方法必须注意两点，首先不同的embedding的权重不一样，其次铆钉节点是稀缺的因此self supervised的方法更有效。</p><p>为了减轻不同类型表示之间的不兼容性，我们首先定义一个由每种嵌入类型构造的嵌入对齐矩阵。更详细地说，给定了源网络<span class="math inline">\(Z_s^{(l)}\)</span>和由表示学习技术l生成的目标网络<span class="math inline">\(Z^{l}_t\)</span>的嵌入，不同embedding层级的对齐矩阵就可以计算为：<span class="math inline">\(S^{(l)}=Z_s^{(l)}Z_t^{(l)T}\)</span>，<span class="math inline">\(S∈R^{n_s \times n_t}\)</span></p><p>最终的S矩阵计算为：<img src="../images/NAME/S矩阵计算.png"></p><p>这种方法的挑战之一是确定每种嵌入技术的优化重要性因子，因为这些因素随着输入网络的属性而变化。使用网络增强来自动优化重要性因素。</p><p>基于扰动的网络增强。对于<span class="math inline">\(G_p=(V_p,A_p,F_p)\)</span>，有<span class="math inline">\(A_p=PAP^T\)</span>，P是任取的置换矩阵。按照之前的文章那样添加属性还有拓扑噪声。</p><p>适应性注意共识机制。我们利用增强机制来使模型能够反映重要的嵌入。给定原始图及其扰动版本，一个好的模型应该在两个版本之间保持相似的嵌入，并完美地对齐网络节点。考虑到这一点，我们通过优化以下函数来学习重要因素：</p><p><img src="../images/NAME/优化函数.png"></p><p>最大化对齐分数，也就是anchor links的<span class="math inline">\((v_i^G,v_i^{G^*})\)</span>。由于源网络和目标网络是同构的，我们只选择源网络作为具有代表性的原始图。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="论文笔记" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>MC2</title>
    <link href="http://yoursite.com/2022/03/23/MC2/"/>
    <id>http://yoursite.com/2022/03/23/MC2/</id>
    <published>2022-03-22T19:49:26.000Z</published>
    <updated>2022-05-30T02:08:31.233Z</updated>
    
    <content type="html"><![CDATA[<p><a id="more"></a></p><h1 id="mc2unsupervised-multiple-social-network-alignment">MC2:Unsupervised Multiple Social Network Alignment</h1><p>来源：ICBD2019</p><!-- more --><p>网络对齐任务的定义为：给定一个社交网络集合<span class="math inline">\(\left \{ S^{(m)}\right\}^M_{m=1}\)</span>没有任何的监督信息，找到一个<span class="math inline">\(Ф^{(m)}\)</span>，将社交网络账号与其拥有者对应来找到属于同一个用户的社交帐号，因此：</p><p><img src="../images/MC2/定义.png"></p><p><img src="../images/MC2/参数.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="论文笔记" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Balancing consistency and disparity in network alignment笔记</title>
    <link href="http://yoursite.com/2022/03/15/Balancing-consistency-and-disparity-in-network-alignment%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2022/03/15/Balancing-consistency-and-disparity-in-network-alignment%E7%AC%94%E8%AE%B0/</id>
    <published>2022-03-15T15:51:02.000Z</published>
    <updated>2022-03-22T01:10:33.117Z</updated>
    
    <content type="html"><![CDATA[<h1 id="论文笔记">论文笔记</h1><a id="more"></a><p>最近的一些基于嵌入的方法可以通过采样负对齐对来在某种程度上产生对齐差异。在不同的甚至相互竞争的负抽样分布设计下，一些方法提倡正相关，这可能会导致假阴性样本错误地违反对齐一致性，而其他节点则支持负相关或均匀分布，这可能对学习有意义的嵌入贡献很小。本论文作者揭开了各种网络对齐方法背后的内在关系，以及这些相互竞争的抽样设计原则之间的内在关系。具体地说，在模型设计方面，我们从理论上揭示了一种特殊的图卷积网络模型与传统的基于一致性的对齐方法之间的密切联系。对于模型训练，我们量化了基于采样分布的网络对齐的嵌入学习的风险。提出了NEXTALIGN在对齐一致性和差异性之间达到了平衡。</p><p>基于对齐一致性的方法可能会导致局部邻域内的对齐的过度平滑问题，并且无法区分正确的对齐。嵌入的方法通过节点嵌入来推断节点对齐，可以在一定程度上合并对齐差异，并通过引入负对齐节点对来改善过平滑问题。通过负采样，学习到的节点嵌入可以潜在地使局部邻域内的对齐更加可分离（即对齐视差）。</p><p><em>过平滑问题：如果在GNN的运算输出的时候，得到了所有节点或者大部分节点的输出向量差不多，那么就认为出现了过平滑。过平滑出现的原因有：1.数据层面的问题，2.图卷积层堆叠的非常深，导致过平滑问题，3.图卷积核的问题</em></p><p>那么什么样的节点算是好的负采样的节点呢？</p><p>良好的负样本应该区分可能误导对齐的锚定链接和近节点对，同时不违反整体对齐的一致性。此外，这些负样本应该告知模型学习有意义的网络对齐嵌入。这些方法也有局限性：由于是单个网络的嵌入，基于正相关的抽样可能导致假的negative对齐对（即(c,f)作为anchor link (b,y)的负对齐对）这导致不正确的对齐违反对齐一致性。另一方面，利用先前定义的分布以及那些与正采样分布呈负相关的样本可能会对遥远或不同的节点对进行采样（也就是(e,h））可能并不会对学习有意义的embedding有多大的作用。</p><p><img src="../images/KDD21/图1.png"></p><ul><li>model design：我们从理论上证明了由图卷积网络所推断出的对齐类似于基于FINAL一致性的对齐方法的半监督变体。这激发了一个特定的图卷积网络模型，可以保持对齐的一致性。</li><li>model training：我们提供了一个引理，它表明通过期望损失和经验损失学习到的节点嵌入的内积之间的均方误差可以通过不同的抽样分布来量化。为了减少上述竞争设计相互兼容的误差，我们设计了一种新的对齐评分函数，为所提出的抽样策略铺平了道路。</li></ul><p><img src="../images/KDD21/本文参数.png"></p><p><span class="math inline">\(n_1=|V_1|,n_2=|V_2|\)</span>表示节点数<span class="math inline">\(X_1∈R^{d_0 \times n_1},X_2∈R^{d_0 \times n_2}\)</span>。</p><p><span class="math inline">\(\mathcal{L}_{1}=\left\{a \mid \exists x \in \mathcal{V}_{2}\right.\)</span>, s.t., <span class="math inline">\(\left.(a, x) \in \mathcal{L}\right\}\)</span>表示网络1中的anchor节点</p><ul><li>问题1：semi-supervised network alignment</li><li>given：<span class="math inline">\(G_1= \left\{V_1,A_1,X_1\right\},G_2=\left\{V_1,A_2,X_2\right\}\)</span>以及anchor links集合L。</li><li>output：对齐矩阵S，其中<span class="math inline">\(S(a,x)\)</span>表明节点a和x对齐的程度</li></ul><p>对于没有节点特征的网络，使用one-hot embedding编码输入节点特征。我们可以通过将两个锚定节点合并为单个节点，将输入网络G1、G2集成到一个世界观网络G中。最终，G具有G1和G2中的非anchor nodes，以及独特的锚定节点作为G的节点。G1和G2中所有的边都在G中共存。通过在这个世界观网络G中学习节点嵌入，我们可以自然地在两个相应的锚点上共享唯一的嵌入。</p><p>对齐一致性（FINAL）</p><p><span class="math inline">\(\min _{\mathrm{S}} \sum_{a, b, x, y}\left[\frac{\mathrm{S}(a, x)}{\sqrt{\left|\mathcal{N}_{1}(a)\right|\left|\mathcal{N}_{2}(x)\right|}}-\frac{\mathrm{S}(b, y)}{\sqrt{\left|\mathcal{N}_{1}(b)\right|\left|\mathcal{N}_{2}(y)\right|}}\right]^{2} \mathrm{~A}_{1}(a, b) \mathrm{A}_{2}(x, y)\)</span></p><p>其中<span class="math inline">\(N_1(a),N_2(x)\)</span>表示节点a和节点x的邻居。<span class="math inline">\(A_1=A_1&#39;,A_2=A_2&#39;\)</span>,当节点b和y是节点a和x的邻居时，公式1会导致<span class="math inline">\(S(a,x)和S(b,y)\)</span>之间的小的不同。另一个角度解释公式1，给定一个anchor链接<span class="math inline">\((a,x)\)</span>具有很高的<span class="math inline">\(S(a,x)\)</span>，这将导致<span class="math inline">\(S(b,y)\)</span>也很高，公式1自然地将所有相邻的节点对视为（𝑎，𝑥）的正对齐对.</p><p>公式1的解：</p><p><span class="math inline">\(\mathrm{S}^{t}=\tilde{\mathrm{A}}_{1} \mathrm{~S}^{t-1} \tilde{\mathrm{A}}_{2}^{\prime}\)</span></p><p>其中<span class="math inline">\(\widetilde A_1,\widetilde A_2\)</span>是<span class="math inline">\(A_1,A_2\)</span>的对称标准化。</p><p>半监督模型中的anchor links可以被用作正则化，也就是：</p><p><span class="math inline">\(\mathbf{S}^{t}=\alpha \tilde{\mathbf{A}}_{1} \mathbf{S}^{t-1} \tilde{\mathbf{A}}_{2}^{\prime}+(1-\alpha) \mathbf{L}\)</span></p><p>其中α控制的是对齐一致性的重要性。只有当<span class="math inline">\((a,x)\)</span>是anchor links时<span class="math inline">\(L(a,x)=1\)</span>，其他都为0</p><p>为了设计一个模型学习节点嵌入的同时遵循对齐一致性，我们首先证明了由一种特定的无参数消息传递方式的节点embeddings的对齐与半监督的FINAL方法是类似的。证明的关键思想是对等式(3)中使用的矩阵L进行rank-|L|分解和使用分解后的矩阵作为输入节点嵌入。直观上，通过将anchor nodes视为<span class="math inline">\(|L|\)</span>维度的欧氏空间中的landmarks，这个消息传递可以解释为确定所有节点与anchor nodes相关的相对位置。随后，基于其捕获对齐一致性的能力，我们提出了该消息传递的参数化对应物在等式6中。我们将其命名为RelGCN层，然后用它来校准由等式 (5)计算出的节点的相对位置.最终的节点嵌入是通过在这些位置向量上应用一个线性层得到的。</p><p><img src="../images/NEXTALIGN/模型框架.png"></p><p>在模型训练方面，实现权衡的关键思想是通过不同的抽样分布。它背后的直觉如下：给定一个anchor link <span class="math inline">\((a,x)∈L\)</span>如果<span class="math inline">\((b,y)\)</span>被采样为积极的对齐节点对，这将正向激励节点对<span class="math inline">\((b,y)\)</span>和<span class="math inline">\((a,x)\)</span>之间的一致性。相反，如果<span class="math inline">\((b,y)\)</span>被选作负对齐节点对，它们之间的对齐差异是有利的。此外，为了保持同一网络中的局部邻近性，我们还对正上下文对和负上下文对进行了采样。为了设计这些抽样分布，我们首先量化了通过最小化预期损失和经验损失来学习的节点嵌入的内积之间的均方误差。在此基础上，为了在满足不同甚至相互竞争的设计原则的同时，更准确地对高概率节点对的内积进行估计，我们提出了一种新的对齐方法 反映节点嵌入的多个方面的评分功能。</p><h2 id="模型设计">模型设计</h2><p>假设节点a,x的对齐是由<span class="math inline">\(S(a,x)=a&#39;x\)</span>计算的，随后我们有：</p><p><span class="math inline">\(\begin{aligned}\left(\mathbf{a}^{t}\right)^{\prime} \mathbf{x}^{t} &amp;=\mathrm{S}^{t}(a, x)=\tilde{\mathrm{A}}_{1}(a,:) \mathrm{S}^{t-1} \tilde{\mathrm{A}}_{2}(:, x) \\ &amp;=\sum_{b \in \mathcal{N}_{1}(a)} \sum_{y \in \mathcal{N}_{2}(x)} \frac{\left(\mathbf{b}^{t-1}\right)^{\prime} \mathbf{y}^{t-1}}{\sqrt{\left|\mathcal{N}_{1}(a)\right|\left|\mathcal{N}_{1}(b)\right|\left|\mathcal{N}_{2}(x)\right|\left|\mathcal{N}_{2}(y)\right|}} \\ &amp;=\sum_{b \in \mathcal{N}_{1}(a)} \frac{\left(\mathbf{b}^{t-1}\right)^{\prime}}{\sqrt{\left|\mathcal{N}_{1}(a)\right|\left|\mathcal{N}_{1}(b)\right|}} \sum_{y \in \mathcal{N}_{2}(x)} \frac{\mathbf{y}^{t-1}}{\sqrt{\left|\mathcal{N}_{2}(x)\right|\left|\mathcal{N}_{2}(y)\right|}} \end{aligned}\)</span></p><p>其中<span class="math inline">\(b^{t-1}\)</span>表示节点b的第<span class="math inline">\((t-1)\)</span>层的节点嵌入。可以看见，第t层的计算对齐矩阵<span class="math inline">\(S^t(a,x)\)</span>相当于通过应用不带参数的普通GCN来更新节点嵌入。</p><p><span class="math inline">\(\mathbf{a}^{t}=\sum_{b \in \mathcal{N}_{1}(a)} \frac{\mathbf{b}^{t-1}}{\sqrt{\left|\mathcal{N}_{1}(a)\right|\left|\mathcal{N}_{1}(b)\right|}}, \mathbf{x}^{t}=\sum_{y \in \mathcal{N}_{2}(x)} \frac{\mathbf{y}^{t-1}}{\sqrt{\left|\mathcal{N}_{2}(x)\right|\left|\mathcal{N}_{2}(y)\right|}}\)</span></p><p>与一般的GCN模型不同的是，这里并没有使用带有自环的邻域。由于GCN面临的过平滑的问题，对齐可能也会造成过平滑的问题。在锚定链接可用的半监督设置中，我们设计了以下没有参数的消息传递：</p><p><span class="math inline">\(\begin{aligned} \mathbf{u}^{t} &amp;=\sqrt{\alpha} \sum_{b \in \mathcal{N}_{1}(u)} \frac{\mathbf{b}^{t-1}}{\sqrt{\left|\mathcal{N}_{1}(u)\right|\left|\mathcal{N}_{1}(b)\right|}}+\sqrt{1-\alpha} \mathbf{u}^{t-1} \\ \mathbf{v}^{t} &amp;=\sqrt{\alpha} \sum_{y \in \mathcal{N}_{2}(v)} \frac{\mathbf{y}^{t-1}}{\sqrt{\left|\mathcal{N}_{2}(v)\right|\left|\mathcal{N}_{2}(y)\right|}}+\sqrt{1-\alpha} \mathbf{v}^{t-1} \\ \mathbf{a}^{t}=\mathbf{x}^{t} &amp;=\sqrt{\alpha} \sum_{b \in \mathcal{N}_{1}(a)} \frac{\mathbf{b}^{t-1}}{\sqrt{\left|\mathcal{N}_{1}(a)\right|\left|\mathcal{N}_{1}(b)\right|}}+\sqrt{1-\alpha} \mathbf{x}^{t-1} \\ &amp;+\sqrt{\alpha} \sum_{y \in \mathcal{N}_{2}(x)} \frac{\mathbf{y}^{t-1}}{\sqrt{\left|\mathcal{N}_{2}(x)\right|\left|\mathcal{N}_{2}(y)\right|}} \end{aligned}\)</span></p><p>其中节点u和节点v是非铆钉节点，节点a和节点x是铆钉节点。</p><ul><li><p>lemma 1：假设初始的非铆钉节点嵌入是<span class="math inline">\(u^0=v^0=0\)</span>并且铆钉节点是<span class="math inline">\(a^0=x^0=e_i∈R^{|L|}\)</span>其中<span class="math inline">\((a,x)\)</span>是第i个anchor link，<span class="math inline">\(e_i(i)=1 \ and \ e_i(j)=0 j≠i\)</span>。然后更新公式(5)一次，对齐计算与公式(3)一样直到额外的网络内部接近度和可能的缩放项。</p></li><li><p>证明：给定<span class="math inline">\(|L|=L\)</span>个铆钉节点，我们可以对L进行L维没有误差的矩阵分解为<span class="math inline">\(L=L&#39;_1L_2\)</span>。由于<span class="math inline">\(L(a,x)=1 \ if\  (a,x)∈L\)</span>，我们有<span class="math inline">\(L_1(:,a)=a^0=e_i,L_2(:,x)=x^0=e_i\)</span>。对于非铆钉节点，我们有<span class="math inline">\(L_1(:u)=u^0=0,L_2(:,v)=v^0=0\)</span>，在初始化嵌入矩阵为<span class="math inline">\(L_1,L_2\)</span>之后，对齐由使用等式(5)更新的嵌入之间的内积计算 .</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;论文笔记&quot;&gt;论文笔记&lt;/h1&gt;
    
    </summary>
    
    
    
      <category term="论文笔记" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>ORIGIN笔记</title>
    <link href="http://yoursite.com/2022/03/09/ORIGIN%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2022/03/09/ORIGIN%E7%AC%94%E8%AE%B0/</id>
    <published>2022-03-09T10:56:04.000Z</published>
    <updated>2022-05-30T02:09:01.607Z</updated>
    
    <content type="html"><![CDATA[<h1 id="origin-non-rigid-network-alignment">ORIGIN: Non-Rigid Network Alignment</h1><p><a id="more"></a></p><p>大多数现有的方法都是显式或隐式地将对齐矩阵作为一个线性转换来映射一个网络到另一个网络,并且可能会忽略跨网络之间复杂的对齐关系。基于节点表示学习的对齐方法受到不同网络节点表示的影响。这篇论文提出了一个统一的半监督深度模型ORIGN，同时找到non-rigid网络对齐，并以互惠互利的方式学习多个网络中的节点表示。其关键思想是通过有效的图卷积网络来学习节点表示，这将使我们能够将网络对齐表述为一个点集对齐问题。</p><p>提出的模型有两点好处：（1）节点表示，与现有的图卷积神经网络的在单个网络中聚合节点信息不同，我们可以有效地从多个来源聚合辅助信息，实现了far-reaching的节点表示法。（2）网络对齐，在高质量节点表示的指导下，我们提出的non-rigid点集对齐方法克服了线性变换假设的瓶颈。</p><p>现有的方法存在的问题：（1）对齐矩阵S被用作一个线性变换矩阵，因此可能会过度简化网络之间的复杂对齐关系；（2）生成的节点特征向量是高维的，其表示能力可能不足。</p><p>这篇论文假设：网络对齐和节点表示学习是互惠互利的。(1) 网络对齐有利于网络表示学习。如果跨网络的节点是对齐的，一个网络中节点的结构和属性信息可以与另一个网络中的节点集成，作为辅助的信息，导致了一个影响深远的表征学习策略。(2) 网络标识学习有利于网络对齐。以节点表示具有高质量为前提，在非欧几里得空间中（即寻找节点对齐，直接跨网络）可以在欧氏空间转化为点集对齐问题。这自然地增加了通过推断non-grid变换来揭示节点对齐的可能性，而非刚性变换有望导致更准确的对齐。</p><p>首先，为了学习多个网络的节点表示，我们设计了一个新的卷积算子，它可以聚合多源信息。为了对齐节点表示，我们提出了一种半监督的多视图非刚性点集对齐算法，该算法首先学习点集变换函数，然后基于转换后的节点表示形式的排列推断出对齐。</p><ul><li>问题定义：第一个提出non-rigid network alignment problem</li><li>模型和算法：半监督学习算法，同时学习不同网络的节点表示，以及揭示了跨多个网络的non-grid网络对准</li><li>预测：</li></ul><h2 id="non-rigid-network-alignment-problem">non-rigid network alignment problem</h2><p>不像线性变换或仿射变换被局限于一些显式表达的变换函数，非刚性变换具有更灵活的灵活性来揭示点集之间的复杂对齐，因为它不需要任何特定形式的变换函数。我们将网络对齐问题转化为点集对齐问题。也就是说，给定已知的非欧几里得数据的输入网络,我们旨在（1）表示欧几里得空间中的节点，使它们可以自然地视为点集，（2）通过推断不同网络之间的非刚性变换，对齐不同网络（即不同的点集）中的节点。</p><ul><li>问题1： non-rigid network alignment</li></ul><p>给定：无向网络<span class="math inline">\(G_1= {V_1,A_1,X^0}\)</span>和<span class="math inline">\(G_2={V_2,A_2,Y^0}\)</span>，其中V表示节点集，<span class="math inline">\(|V_1|=n_1,|V_2|=n_2\)</span>，A是邻接矩阵，<span class="math inline">\(X^0,Y^0\)</span>是输入的属性矩阵。（2）标注的节点对集合<span class="math inline">\(L^+={(u_{l_i},v_{l_i})}|i=1,2,...,L\)</span>，其中节点<span class="math inline">\(u_{l_i}\)</span>和节点<span class="math inline">\(v_{l_i}\)</span>是作为先验知识的两个已知的在两个网络中对齐的点。一个可选的先验跨网络节点相似度矩阵H。</p><p>找到：<span class="math inline">\(n_1 \times n_2\)</span>软对齐矩阵S，其中<span class="math inline">\(S(u,v)\)</span>表示<span class="math inline">\(G_1\)</span>中的节点u和<span class="math inline">\(G_2\)</span>中的节点v的对其程度。节点表示矩阵Z,Y</p><p>如果没有关于跨网络节点的相似度矩阵的先验知识，我们也可以用一些启发式的方法来构造H，如节点度相似度。</p><h2 id="preliminary-graph-convolutional-networks">preliminary: Graph convolutional networks</h2><p>这里作者使用GraphSage来做节点的表示学习，</p><h3 id="graphsage">GraphSage</h3><p>本文提出了一种归纳学习GraphSage框架，通过聚合节点邻居的函数（卷积层），使GCN扩展为归纳学习任务，对未知节点起到泛化作用。</p><ul><li>直推式学习：从特殊到特殊，仅考虑当前数据。在图中学习目标是直接生成当前节点的embedding，例如deepwalk，line等都是使用这种方法，把每个节点embedding作为参数，通过SGD优化。又比如GCN，在训练的过程中使用图的拉普拉斯矩阵进行计算。</li><li>归纳学习：从特殊到一般，目标是在未知数据上也有区分性</li></ul><p>GraphSage的思路就是因为新的节点的增加会改变原来节点的embedding，所以为每个节点得到一个固定的embedding的方法就是不恰当的。而GraphSage方法学到的node embedding是根据node的邻居关系变化而变化的。GraphSage不是试图学习一个图上的所有node的embedding，而是学习一个为每个node产生embedding的映射。</p><ol type="1"><li><p>前向传播</p><p>GraphSage训练了一组aggregator functions，这些函数学习如何从一个顶点的局部邻居聚合特征信息，每个聚合函数从一个顶点的不同的hops或者说不同的搜索深度聚合信息。测试的时候，使用训练好的系统，通过学习到的聚合函数来对完全未见过的节点生成embedding。</p><p><img src="../images/ORIGIN/GraphSage聚合函数.png"></p><p>首先对图中的每个节点进行采样，因为每个节点的度是不一样的，为了计算高效，为每个节点采用固定数量的邻居（图中一跳的邻居数为3，二跳的邻居数为5）。生成目标节点embedding，先聚合2跳邻居的特征，生成一跳邻居embedding，再聚合一跳邻居embedding，生成目标节点embedding，从而获得二跳邻居信息。将embedding作为全连接层的输入，预测目标节点的标签。</p><p><img src="../images/ORIGIN/GraphSage聚合伪代码.jpg"></p><p>4-5行介绍卷积层的操作：聚合与节点v相连的邻居（采样）k-1层的embedding，得到第k层邻居的聚合特征<span class="math inline">\(h_{N(v)}^k\)</span>，与节点第k-1层embedding <span class="math inline">\(h_v^{k-1}\)</span>拼接，并通过全连接层转换，得到节点v在第k层的embedding <span class="math inline">\(h_v^k\)</span>。</p><p>在每次迭代，顶点从它们的局部邻居聚合信息，并且随着这个过程的迭代，顶点会从越来越远的地方获得信息。</p><p>其中K表示的是网络的层数，也代表每个顶点能够聚合的邻接点的跳数，因为每增加一层，可以聚合更远的一层邻居的信息。<span class="math inline">\(x_v\)</span>表示节点v的特征向量,<span class="math inline">\(h_{N(v)}^k\)</span>表示在第k层节点v所有的邻居节点的特征表示。<span class="math inline">\(h_k^v\)</span>表示在第k层节点v的特征表示。<span class="math inline">\(N(v)\)</span>定义为从集合<span class="math inline">\(\left \{ u∈v:(u,V)∈ξ\right\}\)</span>中的固定size的均匀取出，即GraphSage中每一层的节点邻居都是从上一层网络采样的，并不是所有邻居参与，并且采样后的邻居的size是固定的。</p></li><li><p>聚合函数</p><p>平均聚合：首先对邻居embedding中每个维度取平均，然后与目标节点embedding拼接后进行非线性转换：</p><p><span class="math inline">\(h_{N(v)}^{k}=\operatorname{mean}\left(\left\{h_{u}^{k-1}, u \in N(v)\right\}\right)\)</span> <span class="math inline">\(h_{v}^{k}=\sigma\left(W^{k} \cdot C O N C A T\left(h_{v}^{k-1}, h_{N(u)}^{k}\right)\right)\)</span></p><p>归纳式聚合：直接对目标节点和所有邻居embedding中每个维度取平均，然后再非线性转换：</p><p><span class="math inline">\(h_{v}^{k}=\sigma\left(W^{k} \cdot \operatorname{mean}\left(\left\{h_{v}^{k-1}\right\} \cup\left\{h_{u}^{k-1}, \forall u \in N(v)\right\}\right)\right.\)</span></p><p>LSTM聚合：LSTM函数不符合"排序不变量"的性质，需要对邻居随机排序，然后将随机的邻居序列embedding {xt,t∈N(v)}作为LSTM的输入。</p><p>pooling聚合器：首先对每个节点上一层embedding进行非线性变换（等价单个连接层，每一维度表示再某方面的突出表现，依次表示目标节点的embedding</p><p><span class="math inline">\(h_{N(v)}^{k}=\max \left(\left\{\sigma\left(W_{p o o l} h_{u i}^{k}+b\right)\right\}, \forall u_{i} \in N(v)\right)\)</span> <span class="math inline">\(h_{v}^{k}=\sigma\left(W^{k} \cdot C O N C A T\left(h_{v}^{k-1}, h_{N(u)}^{k-1}\right)\right)\)</span></p></li><li><p>无监督和有监督损失设定</p><p>损失函数可以根据具体应用情况，可以使用基于图的无监督损失和有监督损失。</p><p>基于图的无监督损失：希望节点u与邻居v的embedding也相似（对应公式的第一项），而与没有交集的节点<span class="math inline">\(v_n\)</span>不相似（对应公式第二项）。</p><p><span class="math inline">\(J_{\mathcal{G}}\left(\mathbf{z}_{u}\right)=-\log \left(\sigma\left(\mathbf{z}_{u}^{\top} \mathbf{z}_{v}\right)\right)-Q \cdot \mathbb{E}_{v_{n} \sim P_{n}(v)} \log \left(\sigma\left(-\mathbf{z}_{u}^{\top} \mathbf{z}_{v_{n}}\right)\right)\)</span></p><p><span class="math inline">\(z_u\)</span>是生成的embedding，节点v是节点u随机游走访问的邻居，<span class="math inline">\(v_n~P_n(v)\)</span>表示负采样，Q是采样样本数。</p></li><li><p>参数学习</p><p>通过前向传播得到节点u的embedding <img src="https://www.zhihu.com/equation?tex=z_u" alt="[公式]"> ,然后梯度下降（实现使用Adam优化器） <strong>进行反向</strong>传播优化参数 <img src="https://www.zhihu.com/equation?tex=W%5Ek" alt="[公式]"> 和聚合函数内参数。</p></li></ol><p>给定一个网络<span class="math inline">\(G_1\)</span>，每个节点<span class="math inline">\(u∈V_1\)</span>从它的邻居<span class="math inline">\(N_u\)</span>中聚合隐藏的表示并且与当前节点的表示聚合，形式上，被下面的公式计算：</p><p><span class="math inline">\(\begin{aligned} \tilde{\mathbf{x}}_{\mathcal{N}_{u}}^{t} &amp;=\operatorname{AGGREGATE}_{t}\left(\left\{\tilde{\mathbf{x}}_{u^{\prime}}^{t-1}, \forall u^{\prime} \in \mathcal{N}_{u}\right\}\right) \\ \tilde{\mathbf{x}}_{u}^{t} &amp;=\sigma\left(\left[\tilde{\mathbf{x}}_{u}^{t-1} \| \tilde{\mathbf{x}}_{\mathcal{N}_{u}^{t}}\right] \mathbf{W}^{t}\right) \end{aligned}\)</span></p><p><span class="math inline">\([\cdot \| \cdot]\)</span>表示的是两个向量的链接，<span class="math inline">\(\widetilde x_u^{t-1}\)</span>是节点u的表示，<span class="math inline">\(W^t\)</span>表示的是t层的权重矩阵。当t=0时，<span class="math inline">\(\widetilde x_u^0\)</span>被初始化为节点u的输入特征，<span class="math inline">\(\widetilde x_u^0=X^0(u:)\)</span>.GraphSage可以通过均值、LSTM和池聚合器进行实例化。选择MEAN聚合器是因为它的高表示能力和简单性。值得注意的是，GraphSage能够通过均匀采样和替换一个固定大小的相邻节点来进行小批量训练。对于输入网络G1，无监督的GraphSage基于负采样的SkipGram最小化以下损失函数：</p><p><span class="math inline">\(\sum_{u \in \mathcal{V}_{1}} \sum_{\mathcal{G}^{\prime} \in C_{u}}(\tilde{\mathbf{X}})=\log \left(\sigma\left(\tilde{\mathbf{x}}_{u}^{T} \tilde{\mathbf{x}}_{u^{\prime}}\right)\right)-Q \cdot \mathbb{E}_{u_{n}^{\prime} \sim P_{n}\left(u^{\prime}\right)} \log \left(\sigma\left(-\tilde{\mathbf{x}}_{u}^{T} \tilde{\mathbf{x}}_{u_{n}^{\prime}}\right)\right)\)</span></p><p>希望节点u与邻居v的embedding也相似（对应公式的第一项），而与没有交集的节点<span class="math inline">\(v_n\)</span>不相似（对应公式第二项）。</p><h2 id="proposed-model">proposed model</h2><p><img src="../images/ORIGIN/ORIGIN框架.png"></p><h3 id="node-representation-learning-for-multiple-networks">node representation learning for multiple networks</h3><p>aggregation and combination：许多现有的基于空间的图卷积神经网络主要定义两个。（1）intra-aggregation：在单个网络中，从相邻节点（如等式1）聚合节点隐藏表示（或第一层的节点属性信息）（2）intra-combination：将当前隐藏表示与节点的结果聚合表示组合为更新的节点表示。然而，考虑到多个网络可能包含一些相互补充的信息，这可能无法在多个网络场景中提供足够信息的节点表示。除了单个网络的单独图卷积网络(称为内部gcn)，我们提出了一个InterGCN组件，它集成了不同网络的节点表示。如果G1中的节点u与G2中的节点v相似（即，可能是对齐的），我们可以将节点v视为节点u的虚拟邻居。给定两个独立的Intra-GCNs，输出节点的表示<span class="math inline">\(\widetilde x_u, \widetilde y_v∈R^d,u∈V_1,v∈V_2\)</span>，定义cross-network aggregattion为：</p><p><span class="math inline">\(\hat{\mathbf{x}}_{u}=\operatorname{AGGREGATE}_{\text {cross }}\left(\tilde{\mathbf{x}}_{u}\right)=\sum_{v \in \mathcal{V}_{2}} \mathbf{S}(u, v) \tilde{\mathbf{y}}_{v}\)</span> <span class="math inline">\(\hat{\mathbf{y}}_{v}=\operatorname{AGGREGATE}_{\text {cross }}\left(\tilde{\mathbf{y}}_{v}\right)=\sum_{u \in \mathcal{V}_{1}} \mathbf{S}(u, v) \tilde{\mathbf{x}}_{u}\)</span></p><p>S就是对齐矩阵。</p><p>（1）aggregation efficiency：节点对齐矩阵通常是非常的稀疏的，为每个节点计算跨网络aggregation将会产生<span class="math inline">\(O(nd)\)</span>的时间复杂度。（2）aggregation localization：当S是稀疏的时候，公式4和公式5将一个网络中大部分甚至所有节点的表示聚合，也就是全局平滑网络。这将进一步使不同节点的聚合表示不太容易区分。</p><p>为了解决这两个问题，因为给定的是有标签的节点对<span class="math inline">\(L^+={(u_{l_i},v_{l_i})}|i=1,2,...,L\)</span>,表示哪些节点是对齐的，设除了<span class="math inline">\(S(u_{l_i},v_{l_i})=1\)</span>之外的所有<span class="math inline">\(S(u_{l_i})=S(u,v_{l_i})=0\)</span>。此外，对于所有其他的不知道的节点对齐，建议对对齐矩阵S进行降采样如下：对于每个节点<span class="math inline">\(u \notin \left\{ (u_{l_i},v_{l_i})|i=1,2,...,L\right\}\)</span>，我们仅仅保留K个最大的<span class="math inline">\(S(u,v_{q_k})\)</span>列值，并且表示采样的矩阵为<span class="math inline">\(S_1\)</span>，随后将其归一化<span class="math inline">\(\sum_{k=1}^{K} \mathbf{S}_{1}\left(u, v_{q_{k}}\right)=1\)</span>，对于节点v也是一样的得到S2。</p><p>因此，跨网络聚合可以被写作：</p><p><span class="math inline">\(\begin{aligned} \hat{\mathbf{x}}_{u} &amp;=\mathrm{AGGREGATE}_{\mathrm{cross}}\left(\tilde{\mathbf{x}}_{u}\right)=\sum_{k=1}^{K} \mathbf{S}_{1}\left(u, v_{q_{k}}\right) \tilde{\mathbf{y}}_{v_{q_{k}}} \\ \hat{\mathbf{y}}_{v} &amp;=\mathrm{AGGREGATE}_{\mathrm{cross}}\left(\tilde{\mathbf{y}}_{v}\right)=\sum_{k=1}^{K} \mathbf{S}_{2}\left(u_{p_{k}}, v\right) \tilde{\mathbf{x}}_{u_{p_{k}}} \end{aligned}\)</span></p><p>对于跨网络聚合，我们使用</p><p><span class="math inline">\(\begin{aligned} \mathbf{x}_{u} &amp;=\mathrm{COMBINE}_{\text {cross }}\left(\tilde{\mathbf{x}}_{u}, \hat{\mathbf{x}}_{u}\right)=\left[\tilde{\mathbf{x}}_{u} \| \hat{\mathbf{x}}_{u}\right] \mathbf{W}_{c r o s s}+\mathbf{b}_{1} \\ \mathbf{y}_{v} &amp;=\mathrm{COMBINE}_{\text {cross }}\left(\tilde{\mathbf{y}}_{v}, \hat{\mathbf{y}}_{v}\right)=\left[\tilde{\mathbf{y}}_{v} \| \hat{\mathbf{y}}_{v}\right] \mathbf{W}_{c r o s s}+\mathbf{b}_{2} \end{aligned}\)</span></p><p>其中，<span class="math inline">\(x_u,y_v∈R^d\)</span>是由所提出的多层GCN模型的输出节点表示。权重矩阵<span class="math inline">\(W_{cross}∈R^{2d\times d}\)</span>在两个方程中共享，因为它基本上测量如何组合内部gcn和Inter-GCN学习的跨网络组合的表示。</p><p>损失函数：<span class="math inline">\(\mathcal{J}_{G C N}=\mathcal{J}_{\mathcal{G}_{1}}(\mathbf{X})+\mathcal{J}_{\mathcal{G}_{2}}(\mathbf{Y})+\lambda \mathcal{J}_{\text {cross }}(\mathbf{X}, \mathbf{Y})\)</span>，要做的是最小化这个函数。</p><p><span class="math inline">\(J_{cross}\)</span>计算如下：</p><p><span class="math inline">\(\begin{aligned} \mathcal{J}_{\text {cross }}(\mathbf{X}, \mathbf{Y}) &amp;=\sum_{u \in \mathcal{V}_{1}}\left\|\mathbf{x}_{u}-\sum_{k=1}^{K} \mathbf{S}_{1}\left(u, v_{q_{k}}\right) \mathbf{y}_{v_{q_{k}}}\right\|_{2}^{2} \\ &amp;+\sum_{v \in \mathcal{V}_{2}}\left\|\mathbf{y}_{v}-\sum_{k=1}^{K} \mathbf{S}_{2}\left(u_{p_{k}}, v\right) \mathbf{x}_{u_{p_{k}}}\right\|_{2}^{2} \end{aligned}\)</span></p><p>简化<span class="math inline">\(\sum_{k=1}^{K} \mathbf{S}_{1}\left(u, v_{q_{k}}\right) \mathbf{y}_{v_{q_{k}}}\)</span>，重写为：</p><p><img src="../images/ORIGIN/简化重写.png"></p><p>其中，<span class="math inline">\(W_{c_1}\)</span>和<span class="math inline">\(W_{c_2}\)</span>是<span class="math inline">\(W_{cross}\)</span>第一和第二行。基于上述方程，我们可以重新考虑对聚合表示<span class="math inline">\(x_u\)</span>也就是<span class="math inline">\(\sum_{k=1}^{K} \mathbf{S}_{1}\left(u, v_{q_{k}}\right) \mathbf{y}_{v_{q_{k}}}\)</span>的计算分为两步。首先，我们聚合跨网络聚合<span class="math inline">\(\hat {x_u}\)</span>,（2）节点<span class="math inline">\(u_{p_{k&#39;}}\)</span>之间的聚合度结果（表示为<span class="math inline">\(\overline {x}_u\)</span>）,来自于同一个网络<span class="math inline">\(G_1\)</span>权重为<span class="math inline">\(S_1S_2^T(u,u_{u,p_{k&#39;}})\)</span>，随后，其等价于：</p><p><span class="math inline">\(\sum_{k=1}^{K} \mathbf{S}_{1}\left(u, v_{q_{k}}\right) \mathbf{y}_{v_{q_{k}}}=\left[\hat{\mathbf{x}}_{u} \| \overline{\mathbf{x}}_{u}\right] \mathbf{W}_{\text {cross }}+\mathbf{b}_{2}\)</span></p><p>由于<span class="math inline">\(S_1S_2^T\)</span>是一个稀疏矩阵，x¯u的聚合从大多数其他节点全局收集信息，导致了一个较少的象征性的x¯u和更高的计算成本。为了解决这个问题，我们仅保留非零项<span class="math inline">\((S_1S_2^T)(u,u_{pk&#39;})\)</span>，其中<span class="math inline">\(u_{p_k&#39;}\)</span>在一些随机游走中与节点u同时出现的节点.这样，所需要的额外节点表示就只包括在<span class="math inline">\(\widetilde x_{u_{pk&#39;}}\)</span>。</p><h3 id="multi-view-point-set-alignment">multi-view point set alignment</h3><p>因为MultiGCN模型只保留了相同网络内的结构一致性(即等式（3）)和节点表示和它们的聚合表示之间的一致性 e其他网络(即，等式（9）)，节点表示xu和yv在欧几里得空间中仍然很可能不相互接近，即使节点u和节点v应该对齐。为了减轻这一限制，我们建议将节点上的网络对齐问题转化为非刚性点集对齐(PSA)问题，其中每个点都由对应节点在欧几里得空间中的表示所表示。</p><p>特别的，给定一个对齐集合<span class="math inline">\(L^+={(u_{l_i},v_{l_i})}|i=1,2,...,L\)</span>，我们想用一些非刚性向量值变换函数f∈Rd将每个点xuli置换到其对齐的点yvli上，从而实现最大的点重叠。</p><p><span class="math inline">\(\min _{\mathbf{f}} \sum_{i=1}^{L}\left\|\mathbf{x}_{u_{l_{i}}}+\frac{1}{2} \mathbf{f}\left(\mathbf{x}_{u_{l_{i}}}\right)-\mathbf{y}_{v_{l_{i}}}\right\|_{2}^{2}\)</span></p><p>该式子是一个对f没有任何约束的不适定问题。相反，我们通过要求非刚性函数f位于一个特定的函数空间内，即一个再生核希尔伯特空间(RKHS)来建模上述优化问题。</p><p><span class="math inline">\(\min _{\mathbf{f}} \sum_{i=1}^{L}\left\|\mathbf{x}_{u_{l_{i}}}+\frac{1}{2} \mathbf{f}\left(\mathbf{x}_{u_{l_{i}}}\right)-\mathbf{y}_{v_{l_{i}}}\right\|_{2}^{2}+\alpha\|\mathbf{f}\|_{\mathcal{H}}^{2}\)</span></p><p>其中<span class="math inline">\(||f||^2_H\)</span>为H中f的RKHS范数，α为正则化参数。因为每个点(例如，<span class="math inline">\(X_{ul_i}\)</span>)本质上都有两种解释：欧几里得空间中的点（即节点表示）和非里得图空间网络中的相应节点，我们进一步考虑将H分成两个RKHS,<span class="math inline">\(H^1,H^2\)</span>由<span class="math inline">\(H=H^1⊕H^2\)</span>这样:</p><p><span class="math inline">\(\mathcal{H}=\left\{\mathbf{f} \mid \mathbf{f}(\mathbf{x})=\mathbf{f}^{1}(\mathbf{x})+\mathbf{f}^{2}(\mathbf{x}), \mathbf{f}^{1} \in \mathcal{H}^{1}, \mathbf{f}^{2} \in \mathcal{H}^{2}\right\}\)</span></p><p>RKHS范数<span class="math inline">\(||f||^2_H\)</span>可以被重写为：</p><p><span class="math inline">\(\|\mathbf{f}\|_{\mathcal{H}}^{2}=\)</span> <span class="math inline">\(\min _{\substack{\mathbf{f}=\mathbf{f}^{1}+\mathbf{f}^{2} \\ \mathbf{f}^{1} \in \mathcal{H}^{1} \\ \mathbf{f}^{2} \in \mathcal{H}^{2}}} \alpha_{1}\left\|\mathbf{f}^{1}\right\|_{\mathcal{H}^{1}}^{2}+\alpha_{2}\left\|\mathbf{f}^{2}\right\|_{\mathcal{H}^{2}}^{2}+\mu \sum_{j=1}^{n_{1}-L}\left[\mathbf{f}^{1}\left(\mathbf{x}_{u_{r_{j}}}\right)-\mathbf{f}^{2}\left(\mathbf{x}_{u_{r_{j}}}\right)\right]^{2}\)</span></p><p>其中<span class="math inline">\(f^1(x),f^2(x)\)</span>是RKHS <span class="math inline">\(H^1,H^2\)</span>中的转换函数分别对应于点视图和图视图。<span class="math inline">\(u= \left\{ u_{r_j}|j=1,...,n_1-L\right\}\)</span>。表示g1中与G2中未标记节点对齐的未标记节点。<span class="math inline">\(\left[\mathbf{f}^{1}\left(\mathbf{x}_{u_{r_{j}}}\right)-\mathbf{f}^{2}\left(\mathbf{x}_{u_{r_{j}}}\right)\right]^{2}\)</span>规范未标记节点urj上的变换函数<span class="math inline">\(f^1，f^2\)</span>，使其在两个不同的视图中保持一致(即在两个视图中连贯地移动<span class="math inline">\(x_{u_{rj}}\)</span>)。令<span class="math inline">\(H^1,H^2\)</span>与再生核κ1，κ2在一起，那么RKHSH与复制核在一起：</p><p><span class="math inline">\(\kappa\left(u, u^{\prime}\right)=\phi\left(u, u^{\prime}\right)-\mu \mathbf{a}_{u} \mathbf{\Psi} \mathbf{a}_{u^{\prime}}^{T}\)</span></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;origin-non-rigid-network-alignment&quot;&gt;ORIGIN: Non-Rigid Network Alignment&lt;/h1&gt;
&lt;p&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="论文笔记" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>struct2vec笔记</title>
    <link href="http://yoursite.com/2022/03/07/struct2vec%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2022/03/07/struct2vec%E7%AC%94%E8%AE%B0/</id>
    <published>2022-03-07T09:15:46.000Z</published>
    <updated>2022-03-07T07:57:01.578Z</updated>
    
    <content type="html"><![CDATA[<h1 id="struct2vec">struct2vec</h1><p>kdd2017</p><a id="more"></a><p>struc2vec使用层次结构在不同尺度上度量节点相似性，并构建一个多层图来编码结构相似性并为节点生成结构上下文。大多数真实网络中的许多节点特征都表现出很强的同质性，如果隔得比较远的两个节点有相似的结构的话，用deepwalk和node2vec这种表示学习方法是不能够刻画出他们的结构相似性的。</p><p>struct2vec的思想：</p><ul><li>评估节点之间的独立于节点和边缘的结构相似性以及它们在网络中的位置。将考虑具有相似的局部结构的两个节点，独立于网络的位置以及节点的在邻居中的标签。我们的方法也不需要网络连接，并且在不同连接的组件中识别结构相似的节点。</li><li>在层次结构的底部，节点之间的结构一致性只取决于他们的度，在层次结构的顶部，相似性取决于整个网络（从节点的角度来看）</li><li>为节点生成随机上下文，节点是通过遍历加权随机游走历多层图观察到的结构相似的节点序列（而不是原来的网络）。使用随机采样的方式来获取上下文，如果两个节点经常出现在相同的上下文中，表明他们有相类似的结构特征。</li></ul><p>构建显式捕获结构身份的表示是一个相对正交的问题，它没有得到多少的关注。</p><p>好的反应节点的结构特征的方法必须满足以下两个特征：</p><ul><li>嵌入空间的距离得能够反映出节点之间的结构相似性，两个局部拓扑结构相似的节点在嵌入空间的距离应该相近。</li><li>节点的结构相似性不依赖于节点或者边的属性甚至是节点的标签信息。</li></ul><p>strcut2vec包含以下四个步骤：</p><ul><li>对于不同大小的邻域，确定图中每个节点对之间的结构相似性。这在节点之间的结构相似度度量中引入了一个层次结构，提供更多的信息来评估层次结构的每个层次的结构相似性。</li><li>构造一个加权多层图，其中网络中的所有节点都出现在每一层中，每一层对应于衡量结构相似度的层次水平。此外，每一层内每个节点对之间的边权值与其结构相似性成反比。</li><li>使用多层图可以为每个节点生成上下文。特别地，利用多层图上的有偏随机游走来生成节点序列。这些序列很可能包括在结构上更相似的节点。</li><li>应用一种技术，从由节点序列给出的上下文中学习潜在的表示。</li></ul><h2 id="测定结构相似性">1. 测定结构相似性</h2><p>如果两个节点具有相同的度的话，他们的结构是相似的，但是如果邻居也有相同的度的话，他们结构上就更相似。</p><p><span class="math inline">\(G=(V,E)\)</span>表示无向无权网络，<span class="math inline">\(n=|V|\)</span>表示的是节点个数，<span class="math inline">\(K^*\)</span>表示直径。<span class="math inline">\(R_k(u)\)</span>表示的是G中距离（跳数）的节点集正好是k≥0。<span class="math inline">\(R_1(u)\)</span>表示的就是节点u的邻居。<span class="math inline">\(s(S)\)</span>对某个节点集合V中的节点按照度的大小从小到大顺序排序后的序列。</p><p><span class="math inline">\(f_k(u,v)\)</span>为考虑两个节点的k跳邻居时（小于等于k跳的所有邻居均要考虑），两个节点的结构距离：</p><p><span class="math inline">\(f_{k}(u, v)=f_{k-1}(u, v)+g\left(s\left(R_{k}(u)\right), s\left(R_{k}(v)\right)\right)\)</span> <span class="math inline">\(k \geq 0\)</span> and <span class="math inline">\(\left|R_{k}(u)\right|,\left|R_{k}(v)\right|&gt;0\)</span></p><p>其中<span class="math inline">\(g(D_1,D_2)≥0\)</span>测量有序度序列D1和D2之间的距离并且<span class="math inline">\(f_{-1}=0\)</span>。<span class="math inline">\(f_k(u,v)\)</span>是一个单调不降的函数，并且这个函数只有在两个节点同时存在k跳邻居时才有定义。本文使用的时DTW的方法来处理两个序列之间的距离。DTW找到两个序列A和序列B之间的最佳比对，给定距离方程<span class="math inline">\(d(a,b)\)</span>，DTW匹配每个<span class="math inline">\(a∈A\)</span>和<span class="math inline">\(b∈B\)</span>，从而使匹配元素之间的距离之和最小化。这里处理的A和B的序列元素表示度，所以采用方程：<span class="math inline">\(d(a, b)=\frac{\max (a, b)}{\min (a, b)}-1\)</span>，如果a=b那么<span class="math inline">\(d(a,b)=0\)</span></p><p><img src="../images/struct2vec/g公式详解.jpg"></p><p><img src="../images/struct2vec/g函数计算过程.jpg"></p><h2 id="构造上下文图">2. 构造上下文图</h2><p>上一步构造了一个多层加权图来编码节点之间的结构相似性。这一步中，根据这些信息构建一个多层的带权重网络M。其中M的第k层是由节点的k跳邻居所定义的。对于每一层，<span class="math inline">\(k=0,...,k^*\)</span>，是一个带权重的完全图，有<span class="math inline">\(n(n-1)/2\)</span>条边，每条边的权值定义为：<span class="math inline">\(w_{k}(u, v)=e^{-f_{k}(u, v)}, \quad k=0, \ldots, k^{*}\)</span></p><ul><li>这个式子的意思是当任意两个节点的结构相似性越大，其权重越大。</li></ul><p>使用有向边连接层与层，对于第k层的任意节点<span class="math inline">\(u_k\)</span>，都有有向边<span class="math inline">\((u_k,u_{k-1})\)</span>和<span class="math inline">\((u_k,u_{k+1})\)</span>，权重分别为：</p><p><span class="math inline">\(w\left(u_{k}, u_{k+1}\right)=\log \left(\Gamma_{k}(u)+e\right), \quad k=0, \ldots, k-1\)</span> <span class="math inline">\(w\left(u_{k}, u_{k-1}\right)=1, \quad k=1, \ldots, k\)</span></p><p>其中<span class="math inline">\(\Gamma_k(u)\)</span>表示第k层中，所有指向u的边中权重大于该层平均权重的数量。</p><p><span class="math inline">\(\Gamma_{k}(u)=\sum_{v \in V} 1\left(w_{k}(u, v)&gt;\overline{w_{k}}\right)\)</span></p>$ =_{(u, v) (<span class="math display">\[\begin{array}{l}V \\ 2\end{array}\]</span>)} w_{k}(u, v) /(<span class="math display">\[\begin{array}{l}n \\ 2\end{array}\]</span><p>)<span class="math inline">\(表示所有边权重的平均值，\)</span><em>k(u)<span class="math inline">\(实际上表示了第k层中，有多少是与节点u相似的，如果u与很多节点都相似，说明此时一定处于低层次，考虑的信息太少了，那么\)</span><em>k(u)<span class="math inline">\(将会很大，也就是\)</span>w(u_k,u</em>{k+1})&gt;w(u_k,u</em>{k-1})$，对于这种情况，就不适合将本层的节点作为上下文了，应该考虑跳到更高的层去找上下文，所以去更高层的权重更大。</p><p>M有<span class="math inline">\(nk^*\)</span>个节点，最多有<span class="math inline">\(k^{*}\left(\begin{array}{l}n \\ 2\end{array}\right)+2n(k^*-1)\)</span>权重边。</p><h2 id="为节点生成上下文">3. 为节点生成上下文</h2><p>上一步讲的多层网络M的构建是为了寻找合适的上下文，而寻找上下文的方式与deepwalk一样是采用随机游走的方式。M在完全没有使用标签信息的情况下捕获在G中节点的结构相似性。特别的，考虑一个有偏的随机游走，在M周围移动，根据M的权重来随机选择。在每一步之前，随机行走首先决定它将改变层还是在当前层上行走（随概率q&gt;0计算，随机游走保持在当前层中）。</p><p>考虑到它将保留在当前的一层中，在第k层中，从节点u步进到节点v的概率为：</p><p><span class="math inline">\(p_{k}(u, v)=\frac{e^{-f_{k}(u, v)}}{Z_{k}(u)}\)</span></p><p>分母是归一化因子<span class="math inline">\(Z_{k}(u)=\sum_{v \in V \atop v \neq u} e^{-f_{k}(u, v)}\)</span></p><p>请注意，随机游走将更倾向于转移到在结构上更类似于当前顶点的节点上，从而避免与它具有非常不接近的结构相似性的节点。因此，节点的u∈V的上下文很可能具有结构相似的节点，独立于它们的标签和在原始网络G中的位置。</p><p>如果跳层，同样分为两个方向，概率也跟边的权重有关，</p><p><span class="math inline">\(p_{k}\left(u_{k}, u_{k+1}\right)=\frac{w\left(u_{k}, u_{k+1}\right)}{w\left(u_{k}, u_{k+1}\right)+w\left(u_{k}, u_{k-1}\right)}\)</span> <span class="math inline">\(p_{k}\left(u_{k}, u_{k-1}\right)=1-p_{k}\left(u_{k}, u_{k+1}\right)\)</span></p><p>请注意，每次游走在一个层中行走时，它都将当前顶点作为其上下文的一部分，独立于该层。，一个顶点u在第k层中可能有一个给定的上下文（由该层的结构相似性决定），但是在k+1层有这个上下文的子集，因为结构上的相似性不能随着我们移动到更高的层次而增加。是一个跨层的层次上下文的概念，它是所提出的方法的一个基本方面。</p><p>最后，对于每个节点u∈V，我们在第0层中对应的顶点开始随机游走。</p><p>注意到当层数越高，因为考虑的邻域更广，节点间的结构相似性计算越苛刻，因此<strong>在底层计算出两个节点结构相似的，在高层则不一定相似</strong>，并且高层很多节点之间的<img src="https://www.zhihu.com/equation?tex=f_k%28u%2Cv%29" alt="[公式]">可能根本没有定义。这就导致如果随机游走在某个节点u跳到了更高的层，那么在随机游走的序列中，其左边的节点是k层，而右边的节点是k+1层的。而左右两边的取值范围是不同的。换言之，某个节点可能会出现在左边，但不会出现在右边，因为虽然它跟中间那个节点u在k层是相似的，但在k+1层可能无定义或者<img src="https://www.zhihu.com/equation?tex=f_%7Bk%2B1%7D%28u%2Cv%29" alt="[公式]">太大导致随机游走在k=1层走到这个节点的概率几乎可以忽略。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;struct2vec&quot;&gt;struct2vec&lt;/h1&gt;
&lt;p&gt;kdd2017&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="论文笔记" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>REGAL笔记</title>
    <link href="http://yoursite.com/2022/03/02/REGAL%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2022/03/02/REGAL%E7%AC%94%E8%AE%B0/</id>
    <published>2022-03-02T15:39:04.000Z</published>
    <updated>2022-05-30T02:09:21.125Z</updated>
    
    <content type="html"><![CDATA[<h1 id="regal">REGAL</h1><p><a id="more"></a></p><ul><li>来源：CIKM 2018</li></ul><p>representation learning-based graph alignment，利用自动学习节点的表示来对齐跨网络的图，设计了xNetMF，就是一种节点嵌入公式。我们的结果证明了基于无监督表示学习的网络对齐在速度和准确性方面的实用性和前景。</p><p>问题定义如下：</p><ul><li>给定两个图，<span class="math inline">\(G_1,G_2\)</span>分别有<span class="math inline">\(V_1,V_2\)</span>个节点，节点的属性位<span class="math inline">\(A_1,A_2\)</span>，设计一种网络对齐的方法，通过直接学习可比较的节点表示Y1和Y2来对齐节点，其中节点映射：<span class="math inline">\(ψ：V-&gt;V_2\)</span>在网络之间可以被推断出来。</li></ul><p>xNetMF：Cross-Network Matrix Factorization,xNetMF不同于现有的绝大多数网络标识学习方法，（1）依赖于单个图中的节点的临近性，产生在不相交的网络中不可比较的嵌入（2）通常涉及到一些程序上的随机性（如随机游走），这在嵌入学习中引入了差异，即使在一个网络中。相比之下，xNetMF 保留结构相似性，而不是基于邻近性的相似性，允许在单个网络之外进行泛化。</p><p>通过高效、低方差学习节点表示，在此过程中，我们将xNetMF表示为相似矩阵上的矩阵分解，该矩阵包含不相交图中节点之间的结构相似性和属性一致性（如果后者可用）。为了避免显式构造完全相似矩阵，这需要计算多输入网络中节点之间的所有相似性对，我们扩展了大规模内核机器常用的Nyström低秩近似。因此，xNetMF是一种有原则且有效的基于隐式矩阵分解的方法，需要朴素方法的一小部分时间和空间，同时避免了特别的稀疏化启发式 。</p><p><img src="../images/REGAL/模型结构图.png"></p><p>我们是第一个使用SGNS(Skip-gram with negative sanoling)转换节点嵌入来在这样的框架中捕获结构身份。</p><p><img src="../images/REGAL/一些方法的比较.png"></p><p>low-rank matrix approximation：Nyström method for 大而密集的相似矩阵，据我们所知，我们还没有在节点嵌入的上下文中考虑到它。</p><p>论文中的网络节点数可以是不相等的，设置总的节点数是<span class="math inline">\(n=|V_1|+|V_2|\)</span>.</p><p>REGAL的步骤：</p><ul><li>node identity extraction：提取n个节点与结构和属性相关的信息</li><li>efficient similarity-based representation：第二步是通过分解上一步的节点恒等式的相似性矩阵，从概念上得到节点嵌入。为了避免成对节点相似性和显式分解，我们推广了Nyström method 用于低秩矩阵逼近进行隐式相似矩阵分解，通过以下两种方法：1. 只比较每个节点与一个样本的相似性，样本的节点数为：<span class="math inline">\(p&lt;&lt;n\)</span> landmark nodes。2. 利用这些node-to-landmark的相似性，从其低秩近似的分解来构造我们的表示。</li><li>fast node representation alignment：我们通过贪婪地匹配嵌入与一个有效的数据结构来对齐图之间的节点，该数据结构允许从另一个图中快速识别出top-α最相似的嵌入</li></ul><p><img src="../images/REGAL/参数.png"></p><h2 id="node-identity-extraction">1. node identity extraction</h2><p>REGAL表示学习模型xNetMF的目标是以一种可推广到多网络问题的方式定义节点“身份”。</p><ul><li><p>structural identity：我们建议从一个节点的相邻节点的度中了解一个节点的结构特性。为了获得高阶信息，我们还考虑了从原始节点多达k个跳的邻居。</p><p>对于节点<span class="math inline">\(u∈V\)</span>，定义<span class="math inline">\(R_u^k\)</span>在图Gi中恰好距离u k≥0步的节点集。我们想要得到在节点集<span class="math inline">\(R_u^k\)</span>中节点的度信息。一个基本的方法就是将度信息存储在一个D维的向量<span class="math inline">\(d_u^k\)</span>中，D是在原始图G中最大的度，<span class="math inline">\(d_u^k\)</span>的第i个元素<span class="math inline">\(d_u^k(i)\)</span>表示<span class="math inline">\(R_u^k\)</span>中度为i的节点数。真实图具有偏态的度分布。为了防止一个高度节点膨胀这些向量的长度，将节点放在一起变为<span class="math inline">\(b=[log_2D]\)</span>（取上界），因此<span class="math inline">\(d_u^k(i)\)</span>表示的是在<span class="math inline">\(R_u^k\)</span>中的满足<span class="math inline">\(\left\lfloor\log _{2}(\operatorname{deg}(u))\right\rfloor=i\)</span>的节点数。有两点好处：1. 将向量<span class="math inline">\(d_u^k\)</span>的维度缩短，2. 它使它们的条目对噪声引入的不同度的微小变化更加鲁棒，特别是当更多的不同度值组合到一个桶中时。</p></li><li><p>attribute-based identity：给定F节点属性，为每个节点u创建一个F维的向量<span class="math inline">\(f_u\)</span>,<span class="math inline">\(f_u(i)\)</span>表示u节点的第i个特征值。</p></li><li><p>cross-network similarity：</p><p><span class="math inline">\(\operatorname{sim}(u, v)=\exp \left[-\gamma_{s} \cdot\left\|\mathbf{d}_{u}-\mathbf{d}_{v}\right\|_{2}^{2}-\gamma_{a} \cdot \operatorname{dist}\left(\mathbf{f}_{u}, \mathbf{f}_{v}\right)\right]\)</span></p><p>其中γ是控制结构和属性相似性的范围参数，<span class="math inline">\(dist(f_u,f_v)\)</span>基于属性的节点u和v的距离（如果没有属性标签的话就忽略）。<span class="math inline">\(\mathbf{d}_{u}=\sum_{k=1}^{K} \delta^{k-1} \mathbf{d}_{u}^{k}\)</span>是节点u的邻居度向量在K个不同的跳跃上聚合，<span class="math inline">\(δ∈(0,1]\)</span>是更大的跳跃距离的折扣因素；K是需要考虑的最大的跳数距离。因此，我们通过结合多个跳距离上的邻域度分布，在多个层次上比较结构一致性，通过加权来衰减遥远邻域的影响，这是在扩散过程中经常遇到的模式。</p><p>属性向量之间的距离取决于节点属性的类型，对于分类属性，我们建议使用不一致特征的数量作为节点u和节点v的基于属性的距离度量：<span class="math inline">\(\operatorname{dist}\left(\mathbf{f}_{u}, \mathbf{f}_{v}\right)=\sum_{i=1}^{F} \mathbb{1}_{f_{u}}(i) \neq f_{v}(i)\)</span>,其中<span class="math inline">\(\mathbb{1}\)</span>表示的是indicator函数。实值属性可以通过欧几里得距离或余弦距离进行比较。</p></li></ul><p><img src="../images/REGAL/REGAL的流程.png"></p><h2 id="efficient-similarity-based-representation">2. efficient similarity-based representation</h2><p>对于跨网络分析，可以避免随机游走，原因有两点：</p><ul><li><p>（1）它们在表示学习中引入的方差常常使不同网络的嵌入不可可比性；（2）它们可以增加计算费用。为了克服上述问题，我们提出了一种新的基于隐式矩阵分解的方法，该方法利用了一个结合的结构和基于属性的相似性矩阵S。考虑到了在不同社区的亲缘关系。直观上，目标是找到<span class="math inline">\(n \times p\)</span>的矩阵Y和Z，使得<span class="math inline">\(\mathrm{S} \approx \mathrm{YZ}^{\top}\)</span>，其中Y是节点嵌入矩阵，Z并不是我们需要的。</p></li><li><p>limitations of existing approaches：一个naive的方案就是计算两个图内和跨所有节点对之间的结构和基于属性的相似性，形成矩阵S，也就是<span class="math inline">\(S_{ij}=sim(i,j) i,j∈V\)</span>，那么S就可以被显示的分解，例如，通过最小化一个因子分解损失函数，给定S作为输入（例如<span class="math inline">\(\left\|\mathrm{S}-\mathrm{YZ}^{\top}\right\|_{F}^{2}\)</span>）。然而，S的计算和存储都具有n的二次复杂度。</p><p>另一种选择是通过只计算“最重要”的”相似性来创建稀疏相似矩阵，对于每个节点，使用节点度的相似性等启发式方法来选择少量的比较。然而，这种特殊的启发式在噪声环境下可能是脆弱的。对于大多数相似之处，我们根本没有近似值，也不能保证最重要的那些都被计算出来了。</p></li><li><p>reduced <span class="math inline">\(n \times p\)</span> similarity computation：相反，我们提出了一种用低秩矩阵<span class="math inline">\(\widetilde S\)</span>近似全相似矩阵S的原则方法，这个矩阵不会被显式的计算。随机从<span class="math inline">\(G_1,G_2\)</span>选择一个<span class="math inline">\(p&lt;&lt;n\)</span>的landmark节点，并且计算他们与两幅图中所有的n个节点的相似度，使用公式1.这将返回的是一个<span class="math inline">\(n \times p\)</span>的相似度矩阵C。从中我们可以提取出一个<span class="math inline">\(p \times p\)</span>的"landmark-to-landmark"子矩阵W。正如我们下面解释的，这两个矩阵足以近似完整的相似矩阵，并允许我们获得节点嵌入，而无需实际计算和分解S˜。</p></li></ul><p>为了实现这些，我们在node embedding上扩展了Nyström method，它在核机器的随机矩阵方法中有应用。低秩矩阵<span class="math inline">\(\widetilde S\)</span>被定义为<span class="math inline">\(\widetilde S = CW&#39;C^T\)</span>，其中C是<span class="math inline">\(m \times p\)</span>形式的矩阵，从V中采样p个landmark节点，只计算G1和G2的所有n个节点与p个landmark的相似性。W'是W的伪逆矩阵，一个由landmark nodes之间的两两相似性组成的p×p矩阵(它对应于C的一个p行的子集)。我们随机的选择landmarks。</p><p>由于<span class="math inline">\(\widetilde S\)</span>包含对任意一个图中任意一对节点之间的相似性的估计，所以也是n平方的时间。但是并不需要显式的计算<span class="math inline">\(\widetilde S\)</span>。</p><ul><li><p>from similarity to representation：</p><p>定理：给定图<span class="math inline">\(G_1(V_1,E_1)\)</span>以及<span class="math inline">\(G_2(V_2,E_2)\)</span>，它们具有一个<span class="math inline">\(n \times n\)</span>联合组合的结构相似度矩阵和基于属性的相似度矩阵，<span class="math inline">\(S≈\widetilde Y\widetilde Z^T\)</span>，节点的嵌入矩阵Y可以近似被认为是</p><p><span class="math inline">\(\widetilde Y = CUΣ^{1/2}\)</span>，其中C是<span class="math inline">\(n \times p\)</span>的相似性矩阵，<span class="math inline">\(W&#39;=UΣV^T\)</span>是W'全秩奇异值分解，其中W是一个<span class="math inline">\(p \times p\)</span>的landmark-to-landmark相似度矩阵。</p><p>证明：</p><p>给定W'的满秩奇异值分解为<span class="math inline">\(W&#39;=UΣV^T\)</span>，我们可以将公式二重写为<span class="math inline">\(S≈ \widetilde S=C(UΣV^T)C^T=(CUΣ^{1/2}) \cdot (Σ^{1/2}V^TC^T)=\widetilde Y \widetilde Z^T\)</span></p></li></ul><p>现在我们只需要计算一个<span class="math inline">\(n \times p\)</span>的矩阵C，而昂贵的SVD只在其小的子矩阵W上执行。因此，我们可以通过隐式分解来得到节点表示<span class="math inline">\(\widetilde S\)</span>，一个低秩的S近似矩阵。两个输入图g1和g2的p维节点嵌入是<span class="math inline">\(\widetilde Y\)</span>的子集<span class="math inline">\(\widetilde Y_1,\widetilde Y_2\)</span></p><h2 id="fast-node-representation-alignment">3. fast node representation alignment</h2><p>REGAL的最后一步是使用它们的表示来有效地对齐节点，假设有两个节点<span class="math inline">\(u∈V_1\)</span>和<span class="math inline">\(v∈V_2\)</span>可能是对齐的，如果他们的xNetMF是近似的。假设<span class="math inline">\(\widetilde Y_1\)</span>和<span class="math inline">\(\widetilde Y_2\)</span>是节点的p维嵌入矩阵。我们认为（软）对齐的可能性与节点嵌入之间的相似性成正比。因此，我们基于嵌入相似度贪婪地将节点对齐到另一个图中最接近的匹配点，如图2所示。这种方法比基于优化的方法更简单、更快，并且由于高质量的节点特征表示而可以工作。</p><ul><li>data structures for efficient alignment：将嵌入<span class="math inline">\(\widetilde Y_2\)</span>存储在一个k-d树中，对于每个在网络<span class="math inline">\(G_1\)</span>中的节点，我们可以快速的查找到它的embeddings来找到α个离<span class="math inline">\(G_2\)</span>中节点最近的节点。</li></ul><p><img src="../images/REGAL/节点嵌入的距离.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;regal&quot;&gt;REGAL&lt;/h1&gt;
&lt;p&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="论文笔记" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>主动学习</title>
    <link href="http://yoursite.com/2022/03/01/%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2022/03/01/%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0/</id>
    <published>2022-03-01T10:45:44.000Z</published>
    <updated>2022-05-30T02:09:35.533Z</updated>
    
    <content type="html"><![CDATA[<h1 id="主动学习">主动学习</h1><p><a id="more"></a></p><p>有的时候，有类标的数据比较稀少而没有类标的数据是相当丰富的，但是对数据进行人工标注又非常昂贵，学习算法可以主动的提出一些标注请求，将一些经过筛选的数据提交给专家进行标注。</p><p>主动学习流程：</p><ul><li>机器学习模型：包括机器学习模型的训练和预测两部分</li><li>待标注的数据候选集提取：依赖主动学习中的查询函数（Query Function）</li><li>人工标注：专家经验或者业务经验的提炼</li><li>获得候选集的标注数据：获得更有价值的样本数据</li><li>机器学习模型的更新：通过增量学习或者重新学习的方式更新模型，从而将人工标注的信息融入到机器学习的模型中，提升模型效果。</li></ul><p><img src="../images/主动学习/主动学习的流程.jpg"></p><p>通过这个过程，就可以达到人工调优模型的结果，应用的领域包括：</p><ul><li>个性化的垃圾邮件，短信，内容分类</li><li>异常检测</li></ul><p><img src="../images/主动学习/主动学习.webp"></p><p>Q是查询函数，用于从未标注样本池U中查询信息量大的信息，S是督导者，可以为U中样本标注正确的标签。学习者通过少量初始标记样本L开始学习，通过一定的查询函数Q选择出一个或一批最有用的样本，并向督导者询问标签，然后利用获得的新知识来训练分类器和进行下一轮查询。主动学习是一个循环的过程，直至达到某一停止准则为止。</p><p>那么什么样的样本是有用的呢？查询函数的设计的最常用的策略是：不确定性准则和差异性准则。</p><ul><li>不确定性准则：由于信息熵是衡量信息量的概念，也是衡量不确定性的概念。信息熵越大，信息量也就越丰富。不确定性准则就是要找到不确定性最高的样本，因为这些样本包含的丰富信息量，对训练模型是有用的。</li><li>差异性原则：查询函数每次迭代中查询一个或者一批样本，我们希望所查询的样本提供的信息是全面的，各个样本提供的信息不重复不冗余，即样本之间具有一定的差异性。在每轮迭代抽取单个信息量最大的样本加入训练集的情况下，每一轮迭代中模型都被重新训练，以新获得的知识去参与对样本不确定性的评估可以有效地避免数据冗余。但是如果每次迭代查询一批样本，那么就应该想办法保证样本的差异性，避免数据冗余。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;主动学习&quot;&gt;主动学习&lt;/h1&gt;
&lt;p&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
